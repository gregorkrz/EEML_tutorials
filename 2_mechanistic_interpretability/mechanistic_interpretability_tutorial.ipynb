{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WeF2yG1HEKv"
      },
      "source": [
        "# EEML Mechanistic Interpretability Workshop\n",
        "Arthur Conmy and Federico Barbero\n",
        "\n",
        "*This notebook heavily borrows from the ARENA tutorial by Callum McDougall -- if you like this tutorial, we recommend looking further at the [ARENA material](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)*.\n",
        "\n",
        "For those that feel like they could brush up on Transformers, we recommend [this great video tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU) by Neel Nanda. A lot of the material in this notebook has been built by Neel so a huge shoutout to him.\n",
        "\n",
        "The tutorial is designed to introduce you to the core concepts of mechanistic interpretability using the [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) library. We thank the developers of this library for their efforts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zkwFgtGHEKw"
      },
      "source": [
        "Most of the sections are constructed in the following way:\n",
        "\n",
        "1. A particular feature of TransformerLens is introduced.\n",
        "2. You are given an exercise, in which you have to apply the feature.\n",
        "\n",
        "Each exercise will have a difficulty and importance rating out of 5, as well as an estimated maximum time you should spend on these exercises and sometimes a short annotation. You should interpret the ratings & time estimates relatively (e.g. if you find yourself spending about 50% longer on the exercises than the time estimates, adjust accordingly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_JcwWixHEKw"
      },
      "source": [
        "## Content & Learning Objectives\n",
        "\n",
        "### 1️⃣ TransformerLens: Introduction\n",
        "\n",
        "This section is designed to get you up to speed with the TransformerLens library. You'll learn how to load and run models, and learn about the shared architecture template for all of these models.\n",
        "\n",
        "##### Learning Objectives\n",
        "\n",
        "> - Load and run a `HookedTransformer` model\n",
        "> - Understand the basic architecture of these models\n",
        "> - Use the model's tokenizer to convert text to tokens, and vice versa\n",
        "> - Know how to cache activations, and to access activations from the cache\n",
        "> - Use `circuitsvis` to visualise attention heads\n",
        "\n",
        "### 2️⃣ TransformerLens: Hooks\n",
        "\n",
        "Next, you'll learn about hooks, which are a great feature of TransformerLens allowing you to access and intervene on activations within the model. We will mainly focus on the basics of hooks and using them to access activations. You will also build some tools to perform logit attribution within your model, so you can identify which components are responsible for your model's performance on certain tasks.\n",
        "\n",
        "##### Learning Objectives\n",
        "\n",
        "> - Understand what hooks are, and how they are used in TransformerLens\n",
        "> - Use hooks to access activations, process the results, and write them to an external tensor\n",
        "> - Build tools to perform attribution, i.e. detecting which components of your model are responsible for performance on a given task\n",
        "> - Understand how hooks can be used to perform basic interventions like **ablation**\n",
        "\n",
        "### 3️⃣ **Bonus:** Replicating [Golden Gate Claude](https://www.anthropic.com/news/golden-gate-claude)!\n",
        "\n",
        "##### Learning Objectives:\n",
        "\n",
        "Finally, you will be replicating one of the first impressive results from the field of mechanistic interpretability: the \"Golden Gate Claude\" experiment. You will learn how to take an LLM and *steer* it such that it loves to talk about the Golden Gate bridge and San Francisco!\n",
        "\n",
        "> - Work with an LLM finetuned for chat interactions (like ChatGPT, Gemini, Claude, ...)\n",
        "> - Get activations from the inside of a chat LLM\n",
        "> - Do a small-scale replication of Anthropic's [Golden Gate Claude](https://www.anthropic.com/news/golden-gate-claude) famous result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSI231BaHEKx"
      },
      "source": [
        "## Setup code\n",
        "\n",
        "If you get an error like this:\n",
        "\n",
        "```\n",
        "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
        "```\n",
        "\n",
        "Select Runtime > Restart (also discoverable with CTRL+Shift+P)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Please run the code in the cells below to setup the notebook*"
      ],
      "metadata": {
        "id": "7ZnKuaFXtT4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f_0YbkSHEKx",
        "cellView": "form",
        "outputId": "b63e10e8-fced-45a1-d691-eacae9858eef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-3994548384.py:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
            "  Cloning https://github.com/callummcdougall/CircuitsVis.git to /tmp/pip-req-build-m1j_7otc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/callummcdougall/CircuitsVis.git /tmp/pip-req-build-m1j_7otc\n",
            "  Resolved https://github.com/callummcdougall/CircuitsVis.git to commit 1e6129d08cae7af9242d9ab5d3ed322dd44b4dd3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformer_lens==2.16.1\n",
            "  Downloading transformer_lens-2.16.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting eindex-callum\n",
            "  Downloading eindex_callum-0.1.2-py3-none-any.whl.metadata (377 bytes)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (1.9.0)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens==2.16.1)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens==2.16.1)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (2.14.4)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens==2.16.1)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting numpy<2,>=1.24 (from transformer_lens==2.16.1)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (0.2.0)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (4.53.2)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens==2.16.1)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (4.14.1)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens==2.16.1) (0.21.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting importlib-metadata<6.0.0,>=5.1.0 (from circuitsvis==0.0.0)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.16.1) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.16.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.16.1) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.16.1) (0.33.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.23.0->transformer_lens==2.16.1) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets>=2.7.1->transformer_lens==2.16.1) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.7.1->transformer_lens==2.16.1) (3.11.15)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==0.0.0) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens==2.16.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens==2.16.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->transformer_lens==2.16.1) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens==2.16.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens==2.16.1) (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.6->transformer_lens==2.16.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6->transformer_lens==2.16.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.6->transformer_lens==2.16.1) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51->transformer_lens==2.16.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51->transformer_lens==2.16.1) (0.21.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens==2.16.1) (2.33.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.7.1->transformer_lens==2.16.1) (1.20.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.16.1) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens==2.16.1) (1.1.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens==2.16.1) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.16.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.16.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.16.1) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens==2.16.1) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens==2.16.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer_lens==2.16.1) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.6->transformer_lens==2.16.1) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.16.1) (5.0.2)\n",
            "Downloading transformer_lens-2.16.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.0/192.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eindex_callum-0.1.2-py3-none-any.whl (8.3 kB)\n",
            "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import pkg_resources\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter1_transformer_interp\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "installed_packages = [pkg.key for pkg in pkg_resources.working_set]\n",
        "if \"transformer-lens\" not in installed_packages:\n",
        "    %pip install transformer_lens==2.16.1 einops eindex-callum jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/ARENA_3.0-{branch}\n",
        "\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaPSs5g0HEKx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import torch as t\n",
        "assert t.cuda.is_available(), \"You are not connected to a GPU. Please use Runtime > Change Runtime Type and choose e.g. T4\"\n",
        "device = \"cuda:0\"\n",
        "\n",
        "import functools\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Callable\n",
        "import circuitsvis as cv\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from eindex import eindex\n",
        "from IPython.display import display\n",
        "from jaxtyping import Float, Int\n",
        "from torch import Tensor\n",
        "from tqdm import tqdm\n",
        "from transformer_lens import (\n",
        "    ActivationCache,\n",
        "    FactoredMatrix,\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig,\n",
        "    utils,\n",
        ")\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "import gc\n",
        "import json\n",
        "import markdown\n",
        "from huggingface_hub import hf_hub_download\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter1_transformer_interp\"\n",
        "section = \"part2_intro_to_mech_interp\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "\n",
        "import part2_intro_to_mech_interp.tests as tests\n",
        "from plotly_utils import hist, imshow, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
        "\n",
        "# Saves computation time, since we don't need it for the contents of this notebook\n",
        "t.set_grad_enabled(False)\n",
        "\n",
        "MAIN = __name__ == \"__main__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJzLSOc2HEKy"
      },
      "source": [
        "# 1️⃣ TransformerLens: Introduction\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Load and run a `HookedTransformer` model\n",
        "> - Understand the basic architecture of these models\n",
        "> - Use the model's tokenizer to convert text to tokens, and vice versa\n",
        "> - Know how to cache activations, and to access activations from the cache\n",
        "> - Use `circuitsvis` to visualise attention heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC4EHyGAHEKy"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3 was announced in 2020, PaLM in 2022, many more LLMs since then), and yet we have no idea how they work nor how to write one ourselves. The goal of mechanistic interpretability is to try to fix this! It is a very young and growing field, and there are a *lot* of open problems - if you would like to help, please try working on one! **For a list of open problems feel free to check out this [doc](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit#) to figure out where to start.**\n",
        "\n",
        "There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **The TransformerLens library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html) -- credit to Nelson Elhage and Chris Olah for building Garcon!\n",
        "\n",
        "The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [Neel Nanda's notebook analysing Indirect Objection Identification](https://github.com/neelnanda-io/TransformerLens/blob/main/Exploratory_Analysis_Demo.ipynb) or [Neel Nanda's recording of himself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoZx1rhdHEKy"
      },
      "source": [
        "## Loading and Running Models\n",
        "\n",
        "TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuZIbTa0HEKy"
      },
      "outputs": [],
      "source": [
        "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_R2TUB9HEKy"
      },
      "source": [
        "### HookedTransformerConfig\n",
        "\n",
        "Alternatively, you can define a config object, then call `HookedTransformer.from_config(cfg)` to define your model. This is particularly useful when you want to have finer control over the architecture of your model. We'll see an example of this in the next section, when we define an attention-only model to study induction heads.\n",
        "\n",
        "Even if you don't define your model in this way, you can still access the config object through the `cfg` attribute of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNUI_U7mHEKy"
      },
      "source": [
        "### 💻 Exercise - inspect your model\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        "> ```\n",
        "\n",
        "Use `gpt2_small.cfg` to find the following, for your GPT-2 Small model:\n",
        "\n",
        "* Number of layers\n",
        "* Number of heads per layer\n",
        "* Maximum context window\n",
        "\n",
        "You might have to check out the documentation page for some of these. If you're in VSCode then you can reach it by right-clicking on `HookedTransformerConfig` and choosing \"Go to definition\". If you're in Colab, then you can read the [GitHub page](https://github.com/neelnanda-io/TransformerLens).\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "The following parameters in the config object give you the answers:\n",
        "\n",
        "```\n",
        "cfg.n_layers == 12\n",
        "cfg.n_heads == 12\n",
        "cfg.n_ctx == 1024\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P19iBl34HEKy"
      },
      "source": [
        "### Running your model\n",
        "\n",
        "Models can be run on a single string or a tensor of tokens (shape: `[batch, position]`, all integers). The possible return types are:\n",
        "\n",
        "* `\"logits\"` (shape `[batch, position, d_vocab]`, floats),\n",
        "* `\"loss\"` (the cross-entropy loss when predicting the next token),\n",
        "* `\"both\"` (a tuple of `(logits, loss)`)\n",
        "* `None` (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpjpOq-3HEKy"
      },
      "outputs": [],
      "source": [
        "model_description_text = \"\"\"## Loading Models\n",
        "\n",
        "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
        "\n",
        "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
        "\n",
        "loss = gpt2_small(model_description_text, return_type=\"loss\")\n",
        "print(\"Model loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wE3c64wHEKy"
      },
      "source": [
        "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model loss: tensor(4.3443, device='cuda:0')</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDow8eR5HEKy"
      },
      "source": [
        "## Transformer architecture\n",
        "\n",
        "The key picture to keep in your head when thinking about the internals of tranformers is this one:\n",
        "\n",
        "![Anthropic_Circuit_Diagram](https://img2024.cnblogs.com/blog/1797571/202503/1797571-20250325133836129-771431847.png)\n",
        "*From Anthropic's [Mathematical Framework For Transformer Circuits paper](https://transformer-circuits.pub/2021/framework/index.html)*.\n",
        "\n",
        "Do not worry about not understanding every part of the transformer in detail! Part of the reason to do workshops with colabs is so that you can help investigate models to figure out confusing details. (Also ask LLMs such as OpenAI o3 or Gemini 2.5 Pro or Claude 4 to help clarify details!)\n",
        "\n",
        "One of the key bits of unusual terminology that Anthropic use in their [Mathematical Framework For Transformer Circuits paper](https://transformer-circuits.pub/2021/framework/index.html) is the reference to the internal current state of the activations as the **Residual Stream**. Each value $x_0, x_i, x_{i+1}, ...$ on the central vertical line in the diagram above is one value of the residual stream. We will use the phrase \"residual stream\" throughout this tutorial.\n",
        "\n",
        "Now, back to TransformerLens. The `HookedTransformer` class is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads:\n",
        "\n",
        "* The weights `W_K`, `W_Q`, `W_V` mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
        "* The weight matrices `W_K`, `W_Q`, `W_V`, `W_O` and activations have separate `head_index` and `d_head` axes, rather than flattening them into one big axis.\n",
        "    * The activations all have shape `[batch, position, head_index, d_head]`.\n",
        "    * `W_K`, `W_Q`, `W_V` have shape `[head_index, d_model, d_head]` and `W_O` has shape `[head_index, d_head, d_model]`\n",
        "* **Important - we generally follow the convention that weight matrices multiply on the right rather than the left.** In other words, they have shape `[input, output]`, and we have `new_activation = old_activation @ weights + bias`.\n",
        "    * Click the dropdown below for examples of this, if it seems unintuitive.\n",
        "\n",
        "<details>\n",
        "<summary>Examples of matrix multiplication in our model</summary>\n",
        "\n",
        "* **Query matrices**\n",
        "    * Each query matrix `W_Q` for a particular layer and head has shape `[d_model, d_head]`.\n",
        "    * So if a vector `x` in the residual stream has length `d_model`, then the corresponding query vector is `x @ W_Q`, which has length `d_head`.\n",
        "* **Embedding matrix**\n",
        "    * The embedding matrix `W_E` has shape `[d_vocab, d_model]`.\n",
        "    * So if `A` is a one-hot-encoded vector of length `d_vocab` corresponding to a particular token, then the embedding vector for this token is `A @ W_E`, which has length `d_model`.\n",
        "\n",
        "</details>\n",
        "\n",
        "The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhh1NBvFHEKy"
      },
      "source": [
        "### Parameters and Activations\n",
        "\n",
        "It's important to distinguish between parameters and activations in the model.\n",
        "\n",
        "* **Parameters** are the weights and biases that are learned during training.\n",
        "    * These don't change when the model input changes.\n",
        "    * They can be accessed directly from the model, e.g. `model.W_E` for the embedding matrix.\n",
        "* **Activations** are temporary numbers calculated during a forward pass, that are functions of the input.\n",
        "    * We can think of these values as only existing for the duration of a single forward pass, and disappearing afterwards.\n",
        "    * We can use hooks to access these values during a forward pass (more on hooks later), but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
        "    * Attention scores and patterns are activations (this is slightly non-intuitve because they're used in a matrix multiplication with another activation).\n",
        "    * All values of the residual stream are activations, for example.\n",
        "\n",
        "The link below shows a diagram of a single layer (called a `TransformerBlock`) for an attention-only model with no biases. Each box corresponds to an **activation** (and also tells you the name of the corresponding hook point, which we will eventually use to access those activations). The red text below each box tells you the shape of the activation (ignoring the batch dimension). Each arrow corresponds to an operation on an activation; where there are **parameters** involved these are labelled on the arrows.\n",
        "\n",
        "[Link to diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg)\n",
        "\n",
        "The next link is to a diagram of a `TransformerBlock` with full features (including biases, layernorms, and MLPs). Don't worry if not all of this makes sense at first - we'll return to some of the details later. As we work with these transformers, we'll get more comfortable with their architecture.\n",
        "\n",
        "[Link to diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/full-merm.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmpggDJOHEKy"
      },
      "source": [
        "A few shortcuts to make your lives easier when using these models:\n",
        "\n",
        "* You can index weights like `W_Q` directly from the model via e.g. `model.blocks[0].attn.W_Q` (which gives you the `[nheads, d_model, d_head]` query weights for all heads in layer 0).\n",
        "    * But an easier way is just to index with `model.W_Q`, which gives you the `[nlayers, nheads, d_model, d_head]` tensor containing **every** query weight in the model.\n",
        "* Similarly, there exist shortcuts `model.W_E`, `model.W_U` and `model.W_pos` for the embeddings, unembeddings and positional embeddings respectively.\n",
        "* With models containing MLP layers, you also have `model.W_in` and `model.W_out` for the linear layers.\n",
        "* The same is true for all biases (e.g. `model.b_Q` for all query biases)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQR99CUZHEKy"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "This section assumes that you understand tokenization. If you would like to learn about it (highly recommended as it is a very important topic), there is this [excellent video](https://www.youtube.com/watch?v=zduSFxRajkE) by Andrej Karpathy that dives into great detail on the topic.\n",
        "\n",
        "The tokenizer is stored inside the model, and you can access it using `model.tokenizer`. There are also a few helper methods that call the tokenizer under the hood, for instance:\n",
        "\n",
        "* `model.to_str_tokens(text)` converts a string into a list of tokens-as-strings (or a list of strings into a list of lists of tokens-as-strings).\n",
        "* `model.to_tokens(text)` converts a string into a tensor of tokens.\n",
        "* `model.to_string(tokens)` converts a tensor of tokens into a string.\n",
        "\n",
        "Examples of use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw70xrMZHEKy"
      },
      "outputs": [],
      "source": [
        "print(gpt2_small.to_str_tokens(\"gpt2\"))\n",
        "print(gpt2_small.to_str_tokens([\"gpt2\", \"gpt2\"]))\n",
        "print(gpt2_small.to_tokens(\"gpt2\"))\n",
        "print(gpt2_small.to_string([50256, 70, 457, 17]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7ziZgg2HEKz"
      },
      "source": [
        "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">['<|endoftext|>', 'g', 'pt', '2']\n",
        "[['<|endoftext|>', 'g', 'pt', '2'], ['<|endoftext|>', 'g', 'pt', '2']]\n",
        "tensor([[50256,    70,   457,    17]], device='cuda:0')\n",
        "<|endoftext|>gpt2</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Alcz8pYHEKz"
      },
      "source": [
        "<details>\n",
        "<summary>Aside - <code><|endoftext|></code></summary>\n",
        "\n",
        "A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt.\n",
        "\n",
        "TransformerLens appends this token by default, and it can easily trip up new users. Notably, **this includes** `model.forward` (which is what's implicitly used when you do eg `model(\"Hello World\")`). You can disable this behaviour by setting the flag `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors.\n",
        "\n",
        "`prepend_bos` is a bit of a hack, and I've gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn't really matter in training (where all inputs are >1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being \"off\" they normally look at the first token. Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first \"real\" token.\n",
        "\n",
        "Further, *some* models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGF-7LmAHEKz"
      },
      "source": [
        "### 💻 Exercise - how many tokens does your model guess correctly?\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to ~10 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Consider the `model_description_text` you fed into your model above. How many tokens did your model guess correctly? Which tokens were correct?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dla6X2qjHEKz"
      },
      "outputs": [],
      "source": [
        "logits: Tensor = gpt2_small(model_description_text, return_type=\"logits\")\n",
        "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
        "\n",
        "# YOUR CODE HERE - get the model's prediction on the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ZikdxKHEKz"
      },
      "source": [
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "Use `return_type=\"logits\"` to get the model's predictions, then take argmax across the vocab dimension. Then, compare these predictions with the actual tokens, derived from the `model_description_text`.\n",
        "\n",
        "Remember, you should be comparing the `[:-1]`th elements of this tensor of predictions with the `[1:]`th elements of the input tokens (because your model's output represents a probability distribution over the *next* token, not the current one).\n",
        "\n",
        "Also, remember to handle the batch dimension (since `logits`, and the output of `to_tokens`, will both have batch dimensions by default).\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Answer - what you should see</summary>\n",
        "\n",
        "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model accuracy: 33/111\n",
        "Correct tokens: ['\\n', '\\n', 'former', ' with', ' models', '.', ' can', ' of', 'ooked', 'Trans', 'former', '_', 'NAME', '`.', ' model', ' the', 'Trans', 'former', ' to', ' be', ' and', '-', '.', '\\n', '\\n', ' at', 'PT', '-', ',', ' model', ',', \"'s\", ' the']\n",
        "</pre>\n",
        "\n",
        "So the model got 33 out of 111 tokens correct. Not too bad!\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "logits: Tensor = gpt2_small(model_description_text, return_type=\"logits\")\n",
        "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
        "\n",
        "true_tokens = gpt2_small.to_tokens(model_description_text).squeeze()[1:]\n",
        "is_correct = prediction == true_tokens\n",
        "\n",
        "print(f\"Model accuracy: {is_correct.sum()}/{len(true_tokens)}\")\n",
        "print(f\"Correct tokens: {gpt2_small.to_str_tokens(prediction[is_correct])}\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bMpJEkAHEKz"
      },
      "source": [
        "**Induction heads** are a special kind of attention head which we'll examine a lot more in coming exercises. They allow a model to perform in-context learning of a specific form: generalising from one observation that token `B` follows token `A`, to predict that token `B` will follow `A` in future occurrences of `A`, even if these two tokens had never appeared together in the model's training data.\n",
        "\n",
        "**Can you see evidence of any induction heads at work, on this text?**\n",
        "\n",
        "<details>\n",
        "<summary>Evidence of induction heads</summary>\n",
        "\n",
        "The evidence for induction heads comes from the fact that the model successfully predicted `'ooked', 'Trans', 'former'` following the token `'H'`. This is because it's the second time that `HookedTransformer` had appeared in this text string, and the model predicted it the second time but not the first. (The model did predict `former` the first time, but we can reasonably assume that `Transformer` is a word this model had already been exposed to during training, so this prediction wouldn't require the induction capability, unlike `HookedTransformer`.)\n",
        "\n",
        "```python\n",
        "print(gpt2_small.to_str_tokens(\"HookedTransformer\", prepend_bos=False))     # --> ['H', 'ooked', 'Trans', 'former']\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rglqrBMnHEKz"
      },
      "source": [
        "## Caching all Activations\n",
        "\n",
        "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out, on the first sentence from the GPT-2 paper.\n",
        "\n",
        "<details>\n",
        "<summary>Aside - a note on <code>remove_batch_dim</code></summary>\n",
        "\n",
        "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it.\n",
        "\n",
        "`gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UPRg0C6HEKz"
      },
      "outputs": [],
      "source": [
        "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
        "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
        "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
        "\n",
        "print(type(gpt2_logits), type(gpt2_cache))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeFoZ6khHEKz"
      },
      "source": [
        "If you inspect the `gpt2_cache` object, you should see that it contains a very large number of keys, each one corresponding to a different activation in the model. You can access the keys by indexing the cache directly, or by a more convenient indexing shorthand. For instance, here are 2 ways to extract the attention patterns for layer 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnpGfni_HEKz"
      },
      "outputs": [],
      "source": [
        "attn_patterns_from_shorthand = gpt2_cache[\"pattern\", 0]\n",
        "attn_patterns_from_full_name = gpt2_cache[\"blocks.0.attn.hook_pattern\"]\n",
        "\n",
        "t.testing.assert_close(attn_patterns_from_shorthand, attn_patterns_from_full_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeVIVif9HEKz"
      },
      "source": [
        "<details>\n",
        "<summary>Aside: <code>utils.get_act_name</code></summary>\n",
        "\n",
        "The reason these are the same is that, under the hood, the first example actually indexes by `utils.get_act_name(\"pattern\", 0)`, which evaluates to `\"blocks.0.attn.hook_pattern\"`.\n",
        "\n",
        "In general, `utils.get_act_name` is a useful function for getting the full name of an activation, given its short name and layer number.\n",
        "\n",
        "You can use the diagram from the **Transformer Architecture** section to help you find activation names.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgzoU5I7HEKz"
      },
      "source": [
        "### 💻 Exercise - verify activations\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        ">\n",
        "> If you're already comfortable implementing things like attention calculations (e.g. having gone through Neel's transformer walkthrough) you can skip this exercise. However, it might serve as a useful refresher.\n",
        "> ```\n",
        "\n",
        "Verify that `hook_q`, `hook_k` and `hook_pattern` are related to each other in the way implied by the diagram. Do this by computing `layer0_pattern_from_cache` (the attention pattern taken directly from the cache, for layer 0) and `layer0_pattern_from_q_and_k` (the attention pattern calculated from `hook_q` and `hook_k`, for layer 0). Remember that attention pattern is the probabilities, so you'll need to scale and softmax appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTU7Zm6QHEKz"
      },
      "outputs": [],
      "source": [
        "layer0_pattern_from_cache = gpt2_cache[\"pattern\", 0]\n",
        "\n",
        "# YOUR CODE HERE - define `layer0_pattern_from_q_and_k` manually, by performing the steps of the attention calculation (dot product, masking, scaling, softmax)\n",
        "# The steps are the following:\n",
        "# 1. Get queries and keys\n",
        "# 2. Perform the einsum (matmul)\n",
        "# 3. Apply the triangular mask\n",
        "# 4. Normalise by the sqrt of the hidden_dim and apply softmax\n",
        "\n",
        "t.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)\n",
        "print(\"Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKAQcwBdHEKz"
      },
      "source": [
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "You'll need to use three different cache indexes in all:\n",
        "\n",
        "* `gpt2_cache[\"pattern\", 0]` to get the attention patterns, which have shape `[nhead, seqQ, seqK]`\n",
        "* `gpt2_cache[\"q\", 0]` to get the query vectors, which have shape `[seqQ, nhead, headsize]`\n",
        "* `gpt2_cache[\"k\", 0]` to get the key vectors, which have shape `[seqK, nhead, headsize]`\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "layer0_pattern_from_cache = gpt2_cache[\"pattern\", 0]\n",
        "\n",
        "q, k = gpt2_cache[\"q\", 0], gpt2_cache[\"k\", 0]\n",
        "seq, nhead, headsize = q.shape\n",
        "layer0_attn_scores = einops.einsum(q, k, \"seqQ n h, seqK n h -> n seqQ seqK\")\n",
        "mask = t.triu(t.ones((seq, seq), dtype=t.bool), diagonal=1).to(device)\n",
        "layer0_attn_scores.masked_fill_(mask, -1e9)\n",
        "layer0_pattern_from_q_and_k = (layer0_attn_scores / headsize**0.5).softmax(-1)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3JtwBEdHEKz"
      },
      "source": [
        "## Visualising Attention Heads\n",
        "\n",
        "A key insight from the Mathematical Frameworks paper is that we should focus on interpreting the parts of the model that are intrinsically interpretable - the input tokens, the output logits and the attention patterns. Everything else (the residual stream, keys, queries, values, etc) are compressed intermediate states when calculating meaningful things. So a natural place to start is classifying heads by their attention patterns on various texts.\n",
        "\n",
        "When doing interpretability, it's always good to begin by visualising your data, rather than taking summary statistics. Summary statistics can be super misleading! But now that we have visualised the attention patterns, we can create some basic summary statistics and use our visualisations to validate them! (Accordingly, being good at web dev/data visualisation is a surprisingly useful skillset! Neural networks are very high-dimensional object.)\n",
        "\n",
        "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on Anthropic's PySvelte library). If you did the previous set of exercises, you'll have seen this library before.\n",
        "\n",
        "We will use the function `cv.attention.attention_patterns`, which takes two important arguments:\n",
        "\n",
        "* `attention`: the attention head patterns, of shape `[n_heads, seq_len, seq_len]`. This consists of the stacked grid of attention probabilities for each head, i.e. `attention[head, d, s]` is the attention probability from destination position `d` to source position `s` in attention head `head`.\n",
        "* `tokens`: List of tokens, which should have the same length as the `seq_len` dimension of `attention`. Make sure you're not accidentally passing in a list with a dummy dimension, or that differs from `seq_len` because of the BOS token!\n",
        "\n",
        "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n",
        "\n",
        "> Note - you can also use the `cv.attention.attention_heads` function, which presents the data in a different way (the syntax is exactly the same as `attention_patterns`). Note, if you display this in VSCode then it may exhibit a bug where the main plot continually shrinks in size - if this happens, you should instead save the HTML (i.e. with `html = cv.attention.attention_heads(...); with open(\"attn_heads.html\", \"w\") as f: f.write(str(html))`) and open the plot in your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfG1HSZ8HEKz"
      },
      "outputs": [],
      "source": [
        "print(type(gpt2_cache))\n",
        "\n",
        "layer = 0\n",
        "attention_pattern = gpt2_cache[\"pattern\", layer]\n",
        "print(attention_pattern.shape)\n",
        "gpt2_str_tokens = gpt2_small.to_str_tokens(gpt2_text)\n",
        "\n",
        "print(\"Layer 0 Head Attention Patterns:\")\n",
        "display(\n",
        "    cv.attention.attention_patterns(\n",
        "        tokens=gpt2_str_tokens,\n",
        "        attention=attention_pattern,\n",
        "        attention_head_names=[f\"L{layer}H{i}\" for i in range(12)],\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4W62-LXHEK3"
      },
      "source": [
        "Hover over heads to see the attention patterns; click on a head to lock it. Hover over each token to see which other tokens it attends to (or which other tokens attend to it - you can see this by changing the dropdown to `Destination <- Source`).\n",
        "\n",
        "🔎 Can you spot an attention head that attends to the previous token only? In this model, you should be able to find one in the first 5 layers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJehrs0ZHEK3"
      },
      "source": [
        "<details>\n",
        "<summary>Other circuitsvis functions - neuron activations</summary>\n",
        "\n",
        "The `circuitsvis` library also has a number of cool visualisations for **neuron activations**. Here are some more of them (you don't have to understand them all now, but you can come back to them later).\n",
        "\n",
        "The function below visualises neuron activations. The example shows just one sequence, but it can also show multiple sequences (if `tokens` is a list of lists of strings, and `activations` is a list of tensors).\n",
        "\n",
        "```python\n",
        "neuron_activations_for_all_layers = t.stack([\n",
        "    gpt2_cache[\"post\", layer] for layer in range(gpt2_small.cfg.n_layers)\n",
        "], dim=1)\n",
        "# shape = (seq_pos, layers, neurons)\n",
        "\n",
        "cv.activations.text_neuron_activations(\n",
        "    tokens=gpt2_str_tokens,\n",
        "    activations=neuron_activations_for_all_layers\n",
        ")\n",
        "```\n",
        "\n",
        "The next function shows which words each of the neurons activates most / least on (note that it requires some weird indexing to work correctly).\n",
        "\n",
        "```python\n",
        "neuron_activations_for_all_layers_rearranged = utils.to_numpy(einops.rearrange(neuron_activations_for_all_layers, \"seq layers neurons -> 1 layers seq neurons\"))\n",
        "\n",
        "cv.topk_tokens.topk_tokens(\n",
        "    # Some weird indexing required here ¯\\_(ツ)_/¯\n",
        "    tokens=[gpt2_str_tokens],\n",
        "    activations=neuron_activations_for_all_layers_rearranged,\n",
        "    max_k=7,\n",
        "    first_dimension_name=\"Layer\",\n",
        "    third_dimension_name=\"Neuron\",\n",
        "    first_dimension_labels=list(range(12))\n",
        ")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8AipX2DHEK5"
      },
      "source": [
        "# 2️⃣ TransformerLens: Hooks\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Understand what hooks are, and how they are used in TransformerLens\n",
        "> - Use hooks to access activations, process the results, and write them to an external tensor\n",
        "> - Build tools to perform attribution, i.e. detecting which components of your model are responsible for performance on a given task\n",
        "> - Understand how hooks can be used to perform basic interventions like **ablation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibJuAFH0HsKP"
      },
      "source": [
        "## Introducing Our Toy Attention-Only Model\n",
        "\n",
        "Here we introduce a toy 2L attention-only transformer trained specifically for today. Some changes to make them easier to interpret:\n",
        "- It has only attention blocks.\n",
        "- The positional embeddings are only added to the residual stream before calculating each key and query vector in the attention layers as opposed to the token embeddings - i.e. we compute queries as `Q = (resid + pos_embed) @ W_Q + b_Q` and same for keys, but values as `V = resid @ W_V + b_V`. This means that **the residual stream can't directly encode positional information**.\n",
        "    - This turns out to make it *way* easier for induction heads to form, it happens 2-3x times earlier - [see the comparison of two training runs](https://wandb.ai/mechanistic-interpretability/attn-only/reports/loss_ewma-22-08-24-11-08-83---VmlldzoyNTI0MDMz?accessToken=8ap8ir6y072uqa4f9uinotdtrwmoa8d8k2je4ec0lyasf1jcm3mtdh37ouijgdbm) here. (The bump in each curve is the formation of induction heads.)\n",
        "    - The argument that does this below is `positional_embedding_type=\"shortformer\"`.\n",
        "- It has no MLP layers, no LayerNorms, and no biases.\n",
        "- There are separate embed and unembed matrices (i.e. the weights are not tied).\n",
        "\n",
        "We now define our model with a `HookedTransformerConfig` object. This is similar to the `Config` object we used in the previous set of exercises, although it has a lot more features. You can look at the documentation page (Right-click, \"Go to Definition\" in VSCode) to seee what the different arguments do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xlzSF_aHsKP"
      },
      "outputs": [],
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    d_model=768,\n",
        "    d_head=64,\n",
        "    n_heads=12,\n",
        "    n_layers=2,\n",
        "    n_ctx=2048,\n",
        "    d_vocab=50278,\n",
        "    attention_dir=\"causal\",\n",
        "    attn_only=True,  # defaults to False\n",
        "    tokenizer_name=\"EleutherAI/gpt-neox-20b\",\n",
        "    seed=398,\n",
        "    use_attn_result=True,\n",
        "    normalization_type=None,  # defaults to \"LN\", i.e. layernorm with weights & biases\n",
        "    positional_embedding_type=\"shortformer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6W3g5VkHsKP"
      },
      "source": [
        "Note that in the last section we had to define a tokenizer explicitly, and passed it into our model. But here, we just pass a tokenizer name, and the model will automatically create a tokenizer for us (under the hood, it calls `AutoTokenizer.from_pretrained(tokenizer_name)`).\n",
        "\n",
        "Below, you'll load in your weights, with some boilerplate code to download your state dict from HuggingFace (you can do this for any model you've uploaded to HuggingFace yourself):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2c0B2ByHsKP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
        "FILENAME = \"attn_only_2L_half.pth\"\n",
        "\n",
        "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3olSNhOHHsKP"
      },
      "source": [
        "Finally, we'll create our model and load in the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VCTqyLFHsKP"
      },
      "outputs": [],
      "source": [
        "model = HookedTransformer(cfg)\n",
        "pretrained_weights = t.load(weights_path, map_location=device, weights_only=True)\n",
        "model.load_state_dict(pretrained_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Z6FVCtHsKP"
      },
      "source": [
        "Use the [diagram at this link](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg) to remind yourself of the relevant hook names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hzCrjMTHsKP"
      },
      "source": [
        "### 💻 Exercise - visualise & inspect attention patterns\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to ~10 minutes on this exercise.\n",
        ">\n",
        "> It's important to be comfortable using circuitsvis, and the cache object.\n",
        "> ```\n",
        "\n",
        "*This exercise should be very quick - you can reuse code from the previous section. You should look at the solution if you're still stuck after 5-10 minutes.*\n",
        "\n",
        "Visualise the attention patterns for both layers of your model, on the following prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSVrv0wOHsKP"
      },
      "outputs": [],
      "source": [
        "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
        "\n",
        "logits, cache = model.run_with_cache(text, remove_batch_dim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwWF9h4sHsKP"
      },
      "source": [
        "*(Note that we've run the model on the string `text`, rather than on tokens like we did previously when creating a cache - this is something that `HookedTransformer` allows.)*\n",
        "\n",
        "Inspect the attention patterns. What do you notice about the attention heads?\n",
        "\n",
        "You should spot three relatively distinctive basic patterns, which occur in multiple heads. What are these patterns, and can you guess why they might be present?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a05wQpKQHsKP"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE - visualize attention\n",
        "# loop through each layer and display the attention patterns as was done with the gpt2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGEI3AtpHsKP"
      },
      "source": [
        "<details>\n",
        "<summary>Aside - what to do if your plots won't show up</summary>\n",
        "\n",
        "A common mistake is to fail to pass the tokens in as arguments. If you do this, your attention patterns won't render.\n",
        "\n",
        "If this isn't the problem, then it might be an issue with the Circuitsvis library.Rather than plotting inline, you can do the following, and then open in your browser from the left-hand file explorer menu of VSCode:\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Discussion of results </summary>\n",
        "\n",
        "We notice that there are three basic patterns which repeat quite frequently:\n",
        "\n",
        "* `prev_token_heads`, which attend mainly to the previous token (e.g. head `0.7`)\n",
        "* `current_token_heads`, which attend mainly to the current token (e.g. head `1.6`)\n",
        "* `first_token_heads`, which attend mainly to the first token (e.g. heads `0.3` or `1.4`, although these are a bit less clear-cut than the other two)\n",
        "\n",
        "The `prev_token_heads` and `current_token_heads` are perhaps unsurprising, because words that are close together in a sequence probably have a lot more mutual information (i.e. we could get quite far using bigram or trigram prediction).\n",
        "\n",
        "The `first_token_heads` are a bit more surprising. The basic intuition here is that the first token in a sequence is often used as a resting or null position for heads that only sometimes activate (since our attention probabilities always have to add up to 1).\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "str_tokens = model.to_str_tokens(text)\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    attention_pattern = cache[\"pattern\", layer]\n",
        "    display(cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgfHjmRuHsKQ"
      },
      "source": [
        "Now that we've observed our three basic attention patterns, it's time to make detectors for those patterns!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKyn77a7HEK5"
      },
      "source": [
        "## What are hooks?\n",
        "\n",
        "One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us e.g. set up careful counterfactuals and causal intervention to easily understand model behaviour.\n",
        "\n",
        "Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it.\n",
        "\n",
        "We do this by adding a **hook function** to that activation, and then calling `model.run_with_hooks`.\n",
        "\n",
        "*(Terminology note - because basically all the activations in our model have an associated hook point, we'll sometimes use the terms \"hook\" and \"activation\" interchangeably.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSjhRuzHEK5"
      },
      "source": [
        "### Hook functions\n",
        "\n",
        "Hook functions take two arguments: `activation_value` and `hook_point`. The `activation_value` is a tensor representing some activation in the model, just like the values in our `ActivationCache`. The `hook_point` is an object which gives us methods like `hook.layer()` or attributes like `hook.name` that are sometimes useful to call within the function.\n",
        "\n",
        "If we're using hooks to edit activations, then the hook function should return a tensor of the same shape as the activation value. But we can also just have our hook function access the activation, do some processing, and write the results to some external variable (in which case our hook function should just not return anything).\n",
        "\n",
        "An example hook function for changing the attention patterns at a particular layer might look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULOYYDBmHEK5"
      },
      "source": [
        "```python\n",
        "def hook_function(\n",
        "    attn_pattern: Float[Tensor, \"batch heads seq_len seq_len\"],\n",
        "    hook: HookPoint\n",
        ") -> Float[Tensor, \"batch heads seq_len seq_len\"]:\n",
        "\n",
        "    # modify attn_pattern (can be inplace)\n",
        "    return attn_pattern\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhkJbxCwHEK5"
      },
      "source": [
        "### Running with hooks\n",
        "\n",
        "Once you've defined a hook function (or functions), you should call `model.run_with_hooks`. A typical call to this function might look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKfyRlH8HEK5"
      },
      "source": [
        "```python\n",
        "loss = model.run_with_hooks(\n",
        "    tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[\n",
        "        ('blocks.1.attn.hook_pattern', hook_function)\n",
        "    ]\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tWcSjjHEK5"
      },
      "source": [
        "Let's break this code down.\n",
        "\n",
        "* `tokens` represents our model's input.\n",
        "* `return_type=\"loss\"` is used here because we're modifying our activations and seeing how this affects the loss.\n",
        "    * We could also return the logits, or just use `return_type=None` if we only want to access the intermediate activations and we don't care about the output.\n",
        "* `fwd_hooks` is a list of 2-tuples of (hook name, hook function).\n",
        "    * The hook name is a string that specifies which activation we want to hook.\n",
        "    * The hook function gets run with the corresponding activation as its first argument.\n",
        "\n",
        "### A bit more about hooks\n",
        "\n",
        "Here are a few extra notes for how to squeeze even more functionality out of hooks. If you'd prefer, you can [jump ahead](#hooks-accessing-activations) to see an actual example of hooks being used, and come back to this section later.\n",
        "\n",
        "<details>\n",
        "<summary>Resetting hooks</summary>\n",
        "\n",
        "`model.run_with_hooks` has the default parameter `reset_hooks_end=True` which resets all hooks at the end of the run (including both those that were added before and during the run). Despite this, it's possible to shoot yourself in the foot with hooks, e.g. if there's an error in one of your hooks so the function never finishes. In this case, you can use `model.reset_hooks()` to reset all hooks.\n",
        "\n",
        "If you don't want to reset hooks (i.e. you want to keep them between forward passes), you can either set `reset_hooks_end=False` in the `run_with_hooks` function, or just add the hooks directly using the `add_hook` method before your forward passes (this way they won't reset automatically).\n",
        "\n",
        "</details>\n",
        "<details>\n",
        "<summary>Adding multiple hooks at once</summary>\n",
        "\n",
        "Including more than one tuple in the `fwd_hooks` list is one way to add multiple hooks:\n",
        "\n",
        "```python\n",
        "loss = model.run_with_hooks(\n",
        "    tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[\n",
        "        ('blocks.0.attn.hook_pattern', hook_function),\n",
        "        ('blocks.1.attn.hook_pattern', hook_function)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "Another way is to use a **name filter** rather than a single name:\n",
        "\n",
        "```python\n",
        "loss = model.run_with_hooks(\n",
        "    tokens,\n",
        "    return_type=\"loss\",\n",
        "    fwd_hooks=[\n",
        "        (lambda name: name.endswith(\"pattern\"), hook_function)\n",
        "    ]\n",
        ")\n",
        "```\n",
        "</details>\n",
        "<details>\n",
        "<summary><code>utils.get_act_name</code></summary>\n",
        "\n",
        "When we were indexing the cache in the previous section, we found we could use strings like `cache['blocks.0.attn.hook_pattern']`, or use the shorthand of `cache['pattern', 0]`. The reason the second one works is that it calls the function `utils.get_act_name` under the hood, i.e. we have:\n",
        "\n",
        "```python\n",
        "utils.get_act_name('pattern', 0) == 'blocks.0.attn.hook_pattern'\n",
        "```\n",
        "\n",
        "Using `utils.get_act_name` in your forward hooks is often easier than using the full string, since the only thing you need to remember is the activation name (you can refer back to the diagram in the previous section for this).\n",
        "</details>\n",
        "<details>\n",
        "<summary>Using <code>functools.partial</code> to create variations on hooks</summary>\n",
        "\n",
        "A useful trick is to define a hook function with more arguments than it needs, and then use `functools.partial` to fill in the extra arguments. For instance, if you want a hook function which only modifies a particular head, but you want to run it on all heads separately (rather than just adding all the hooks and having them all run on the next forward pass), then you can do something like:\n",
        "\n",
        "```python\n",
        "def hook_all_attention_patterns(\n",
        "    attn_pattern: Float[Tensor, \"batch heads seq_len seq_len\"],\n",
        "    hook: HookPoint,\n",
        "    head_idx: int\n",
        ") -> Float[Tensor, \"batch heads seq_len seq_len\"]:\n",
        "    # modify attn_pattern inplace, at head_idx\n",
        "    return attn_pattern\n",
        "\n",
        "for head_idx in range(12):\n",
        "    temp_hook_fn = functools.partial(hook_all_attention_patterns, head_idx=head_idx)\n",
        "    model.run_with_hooks(tokens, fwd_hooks=[('blocks.1.attn.hook_pattern', temp_hook_fn)])\n",
        "```\n",
        "</details>\n",
        "\n",
        "And here are some points of interest, which aren't vital to understand:\n",
        "\n",
        "<details>\n",
        "<summary>Relationship to PyTorch hooks</summary>\n",
        "\n",
        "[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).\n",
        "\n",
        "They also come with a range of other quality of life improvements. PyTorch's hooks are global state, which can be a massive pain if you accidentally leave a hook on a model. TransformerLens hooks are also global state, but `run_with_hooks` tries to create an abstraction where these are local state by removing all hooks at the end of the function (and they come with a helpful `model.reset_hooks()` method to remove all hooks).\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>How are TransformerLens hooks actually implemented?</summary>\n",
        "\n",
        "They are implemented as modules with the identity function as their forward method:\n",
        "\n",
        "```python\n",
        "class HookPoint(nn.Module):\n",
        "    ...\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "```\n",
        "\n",
        "but also with special features for adding and removing hook functions. This is why you see hooks when you print a HookedTransformer model, because all its modules are recursively printed.\n",
        "\n",
        "When you run the model normally, hook modules won't change the model's behaviour (since applying the identity function does nothing). It's only once you add functions to the hook modules (e.g. a function which ablates any inputs into the hook module) that the model's behaviour changes.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okVgyGS_HEK5"
      },
      "source": [
        "## Hooks: Accessing Activations\n",
        "\n",
        "In later sections, we'll write some code to intervene on hooks, which is really the core feature that makes them so useful for interpretability. But for now, let's just look at how to access them without changing their value. This can be achieved by having the hook function write to a global variable, and return nothing (rather than modifying the activation in place).\n",
        "\n",
        "Why might we want to do this? It turns out to be useful for things like:\n",
        "\n",
        "* Extracting activations for a specific task\n",
        "* Doing some long-running calculation across many inputs, e.g. finding the text that most activates a specific neuron\n",
        "\n",
        "Note that, in theory, this could all be done using the `run_with_cache` function we used in the previous section, combined with post-processing of the cache result. But using hooks can be more intuitive and memory efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncyJt4xdHEK6"
      },
      "source": [
        "## Building interpretability tools\n",
        "\n",
        "In order to develop a mechanistic understanding for how transformers perform certain tasks, we need to be able to answer questions like:\n",
        "\n",
        "> *How much of the model's performance on some particular task is attributable to each component of the model?*\n",
        "\n",
        "where \"component\" here might mean, for example, a specific head in a layer.\n",
        "\n",
        "There are many ways to approach a question like this. For example, we might look at how a head interacts with other heads in different layers, or we might perform a causal intervention by seeing how well the model performs if we remove the effect of this head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEcokFOpHEK6"
      },
      "source": [
        "## Hooks: Intervening on Activations\n",
        "\n",
        "Now that we've built some tools to decompose our model's output, it's time to start making causal interventions.\n",
        "\n",
        "### Ablations\n",
        "\n",
        "Let's start with a simple example: **ablation**. An ablation is a simple causal intervention on a model - we pick some part of it and set it to zero. This is a crude proxy for how much that part matters. Further, if we have some story about how a specific circuit in the model enables some capability, showing that ablating *other* parts does nothing can be strong evidence of this.\n",
        "\n",
        "As mentioned in [the glossary](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx), there are many ways to do ablation. We'll focus on the simplest: zero-ablation (even though it's somewhat unprincipled)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeDKXAvgHEK7"
      },
      "source": [
        "### 💻 Exercise - previous token head ablation\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should aim to spend 20-35 mins on this exercise.\n",
        "> ```\n",
        "\n",
        "The code below provides a template for performing zero-ablation on the output vectors at a particular head (i.e. the vectors we get when taking a weighted sum of the value vectors according to the attention probabilities, before projecting them up & adding them back to the residual stream). If you're confused about what different activations mean, you can refer back to [the diagram](https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/small-merm.svg).\n",
        "\n",
        "You need to do 2 things:\n",
        "\n",
        "1. Fill in `head_zero_ablation_hook` so that it performs zero-ablation on the head given by `head_index_to_ablate`.\n",
        "2. Fill in the missing code in the `get_ablation_scores` function (i.e. where you see the `raise NotImplementedError()` line), so that `loss_with_ablation` is computed as the loss of the model after ablating head `head` in layer `layer`.\n",
        "\n",
        "The rest of the `get_ablation_scores` function is designed to return a tensor of shape `(n_layers, n_heads)` containing the increase in loss from ablating each of these heads.\n",
        "\n",
        "A few notes about this function / tips on how to implement it:\n",
        "\n",
        "- You can create a temporary hook function by applying `functools.partial` to the `ablation_function`, fixing the head index to a particular value.\n",
        "- You can use `utils.get_act_name(\"z\", layer)` to get the name of the hook point (to see the full diagram of named hook points and how to get the names, you can refer to the streamlit reference page, which can be found on the left hand sidebar after you navigate to the [homepage](https://arena-chapter1-transformer-interp.streamlit.app/)).\n",
        "- See that `loss_no_ablation` is computed with the `get_log_probs` function, and that we only take the last `seq_len - 1` tokens - this is because we're dealing with sequences of length `2 * seq_len + 1` (a BOS token plus 2 repeated random sequences), and we only care about the loss on the second half of the sequence.\n",
        "- Note that we call `model.reset_hooks()` at the start of the function - this is a useful practice in general, to make sure you've not accidentally left in any hooks that might change your model's behaviour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcCt0pUHEK7"
      },
      "outputs": [],
      "source": [
        "def get_log_probs(\n",
        "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
        ") -> Float[Tensor, \"batch posn-1\"]:\n",
        "    logprobs = logits.log_softmax(dim=-1)\n",
        "    # We want to get logprobs[b, s, tokens[b, s+1]], in eindex syntax this looks like:\n",
        "    correct_logprobs = eindex(logprobs, tokens, \"b s [b s+1]\")\n",
        "    return correct_logprobs\n",
        "\n",
        "def generate_repeated_tokens(\n",
        "    model: HookedTransformer, seq_len: int, batch_size: int = 1\n",
        ") -> Int[Tensor, \"batch_size full_seq_len\"]:\n",
        "    \"\"\"\n",
        "    Generates a sequence of repeated random tokens\n",
        "\n",
        "    Outputs are:\n",
        "        rep_tokens: [batch_size, 1+2*seq_len]\n",
        "    \"\"\"\n",
        "    t.manual_seed(0)  # for reproducibility\n",
        "    prefix = (t.ones(batch_size, 1) * model.tokenizer.bos_token_id).long() # beginning of sequence\n",
        "    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch_size, seq_len), dtype=t.int64)\n",
        "    rep_tokens = t.cat([prefix, rep_tokens_half, rep_tokens_half], dim=-1).to(device)\n",
        "    return rep_tokens\n",
        "\n",
        "def head_zero_ablation_hook(\n",
        "    z: Float[Tensor, \"batch seq n_heads d_head\"],\n",
        "    hook: HookPoint,\n",
        "    head_index_to_ablate: int,\n",
        ") -> None:\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  raise NotImplementedError()\n",
        "\n",
        "def get_ablation_scores(\n",
        "    model: HookedTransformer,\n",
        "    tokens: Int[Tensor, \"batch seq\"],\n",
        "    ablation_function: Callable = head_zero_ablation_hook,\n",
        ") -> Float[Tensor, \"n_layers n_heads\"]:\n",
        "    \"\"\"\n",
        "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output\n",
        "    of each head.\n",
        "    \"\"\"\n",
        "    # Initialize an object to store the ablation scores\n",
        "    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "\n",
        "    # Calculating loss without any ablation, to act as a baseline\n",
        "    model.reset_hooks()\n",
        "    seq_len = (tokens.shape[1] - 1) // 2\n",
        "    logits = model(tokens, return_type=\"logits\")\n",
        "    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()\n",
        "\n",
        "    for layer in tqdm(range(model.cfg.n_layers)):\n",
        "        for head in range(model.cfg.n_heads):\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    return ablation_scores\n",
        "\n",
        "rep_tokens = generate_repeated_tokens(model, seq_len=50, batch_size=10)\n",
        "ablation_scores = get_ablation_scores(model, rep_tokens)\n",
        "tests.test_get_ablation_scores(ablation_scores, model, rep_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFeaUajLHEK7"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def head_zero_ablation_hook(\n",
        "    z: Float[Tensor, \"batch seq n_heads d_head\"],\n",
        "    hook: HookPoint,\n",
        "    head_index_to_ablate: int,\n",
        ") -> None:\n",
        "    z[:, :, head_index_to_ablate, :] = 0.0\n",
        "\n",
        "\n",
        "def get_ablation_scores(\n",
        "    model: HookedTransformer,\n",
        "    tokens: Int[Tensor, \"batch seq\"],\n",
        "    ablation_function: Callable = head_zero_ablation_hook,\n",
        ") -> Float[Tensor, \"n_layers n_heads\"]:\n",
        "    \"\"\"\n",
        "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output\n",
        "    of each head.\n",
        "    \"\"\"\n",
        "    # Initialize an object to store the ablation scores\n",
        "    ablation_scores = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "\n",
        "    # Calculating loss without any ablation, to act as a baseline\n",
        "    model.reset_hooks()\n",
        "    seq_len = (tokens.shape[1] - 1) // 2\n",
        "    logits = model(tokens, return_type=\"logits\")\n",
        "    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()\n",
        "\n",
        "    for layer in tqdm(range(model.cfg.n_layers)):\n",
        "        for head in range(model.cfg.n_heads):\n",
        "            # Use functools.partial to create a temporary hook function with the head number fixed\n",
        "            temp_hook_fn = functools.partial(ablation_function, head_index_to_ablate=head)\n",
        "            # Run the model with the ablation hook\n",
        "            ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[(utils.get_act_name(\"z\", layer), temp_hook_fn)])\n",
        "            # Calculate the loss difference (= negative correct logprobs), only on the last `seq_len` tokens\n",
        "            loss = -get_log_probs(ablated_logits, tokens)[:, -(seq_len - 1) :].mean()\n",
        "            # Store the result, subtracting the clean loss so that a value of zero means no change in loss\n",
        "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
        "\n",
        "    return ablation_scores\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sMz9fqJHEK7"
      },
      "source": [
        "Once you've passed the tests, you can plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KAREgyIHEK7"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    ablation_scores,\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff\"},\n",
        "    title=\"Loss Difference After Ablating Heads\",\n",
        "    text_auto=\".2f\",\n",
        "    width=900,\n",
        "    height=350,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mH6nAAoHEK7"
      },
      "source": [
        "What is your interpretation of these results?\n",
        "\n",
        "<details>\n",
        "<summary>Interpretation</summary>\n",
        "\n",
        "This tells us not just which heads are responsible for writing output to the residual stream that gets us the correct result, but **which heads play an important role in the repetition of a token**.\n",
        "\n",
        "This chart tells us that - for sequences of repeated tokens - head `0.7` is by far the most important in layer 0 (which makes sense, since we observed it to be the strongest \"previous token head\"), and heads `0.4`, `1.4`, `1.10` are also important... These are so-called \"induction heads\".\n",
        "\n",
        "This is a good illustration of the kind of result which we can get from ablation, but **wouldn't be able to get from something like direct logit attribution**, because it isn't a causal intervention.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIct5x2bHEK7"
      },
      "source": [
        "### 💻 Exercise - mean ablation\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should aim to spend 5-15 mins on this exercise.\n",
        "> ```\n",
        "\n",
        "An alternative to zero-ablation is **mean-ablation**, where rather than setting values to zero, we set them to be their mean across some suitable distribution (commonly we'll use the mean over some batch dimension). This can be more informative, because zero-ablation takes a model out of its normal distribution, and so the results from it aren't necessarily representative of what you'd get if you \"switched off\" the effect from some particular component. Mean ablation on the other hand works slightly better (although it does come with its own set of risks). You can read more [here](https://www.neelnanda.io/mechanistic-interpretability/glossary#:~:text=Ablation%20aka%20Knockout) or [here](https://arxiv.org/html/2404.15255v1).\n",
        "\n",
        "You should fill in the `head_mean_ablation_hook` function below, and run the code (also make sure in your previous `get_ablation_scores` function that you were actually using the `ablation_function` rather than hardcoding the zero ablation function, otherwise your code won't work here). You should see that the results are slightly cleaner, with the unimportant heads having values much closer to zero relative to the important heads.\n",
        "\n",
        "**Hard: Can you think of why for instance attention head `0.3` is quite different with mean ablation when compared to zero ablation?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRT1r0P-HEK7"
      },
      "outputs": [],
      "source": [
        "def head_mean_ablation_hook(\n",
        "    z: Float[Tensor, \"batch seq n_heads d_head\"],\n",
        "    hook: HookPoint,\n",
        "    head_index_to_ablate: int,\n",
        ") -> None:\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# More code here...\n",
        "# Feel free to adapt the code from the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP_V9BRHHEK7"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def head_mean_ablation_hook(\n",
        "    z: Float[Tensor, \"batch seq n_heads d_head\"],\n",
        "    hook: HookPoint,\n",
        "    head_index_to_ablate: int,\n",
        ") -> None:\n",
        "    z[:, :, head_index_to_ablate, :] = z[:, :, head_index_to_ablate, :].mean(0)\n",
        "\n",
        "\n",
        "rep_tokens = generate_repeated_tokens(model, seq_len=50, batch_size=10)\n",
        "mean_ablation_scores = get_ablation_scores(model, rep_tokens, ablation_function=head_mean_ablation_hook)\n",
        "tests.test_get_ablation_scores(ablation_scores, model, rep_tokens)\n",
        "\n",
        "imshow(\n",
        "    mean_ablation_scores,\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff\"},\n",
        "    title=\"Loss Difference After Ablating Heads\",\n",
        "    text_auto=\".2f\",\n",
        "    width=900,\n",
        "    height=350,\n",
        ")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3️⃣ **Bonus:** Replicating [Golden Gate Claude](https://www.anthropic.com/news/golden-gate-claude)!\n",
        "\n",
        "##### Learning Objectives:\n",
        "\n",
        "Finally, you will be replicating one of the first impressive results from the field of mechanistic interpretability: the \"Golden Gate Claude\" experiment. You will learn how to take an LLM and *steer* it such that it loves to talk about the Golden Gate bridge and San Francisco!\n",
        "\n",
        "> - Work with an LLM finetuned for chat interactions (like ChatGPT, Gemini, Claude, ...)\n",
        "> - Get activations from the inside of a chat LLM\n",
        "> - Do a small-scale replication of Anthropic's [Golden Gate Claude](https://www.anthropic.com/news/golden-gate-claude) famous result\n",
        "\n",
        "## 3️⃣ Setup\n",
        "\n",
        "We need to load a chat model. We'll use Qwen-3-1.7B from Alibaba. You need to have a T4 GPU (or better) so the model runs fast and fits on your GPU."
      ],
      "metadata": {
        "id": "8k00TXD0QlW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Downloading this model takes a few minutes\n",
        "chat_model = HookedTransformer.from_pretrained_no_processing(\n",
        "    \"Qwen/Qwen3-1.7B\",\n",
        "    torch_dtype=t.bfloat16,\n",
        ").to(\"cuda:0\")\n",
        "\n",
        "# Especially when repeatedly running, garbage collecting can prevent OOMs\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "pDxnk2ePQpXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 Exercise: Running the Model\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴⚪⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Before we can analyze the model's internals, we need to be able to interact with it correctly. The Qwen-3 model uses a specific chat template. We have provided a helper function, `format_qwen3`, that constructs this for you.\n",
        "\n",
        "Your first task is to use this function to **create a prompt and generate a response from the model**. This will familiarize you with the basic input/output loop. You can use [the TransformerLens docs](https://transformerlensorg.github.io/TransformerLens/) to help with this (see also the hint). Check that model's response makes sense to confirm that you've implemented the task correctly.\n",
        "\n",
        "The `## Tokenization` section from earlier may be helpful.\n"
      ],
      "metadata": {
        "id": "XW4qIQZIpk7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IM_START, IM_END = \"<|im_start|>\", \"<|im_end|>\"\n",
        "\n",
        "def get_qwen3_str(\n",
        "    user_prompt: str,\n",
        "    assistant_prompt: str | None = None,\n",
        "    # No system instruction seens to work badly, sadly\n",
        "    system_prompt: str | None = (\n",
        "        \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
        "    ),\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Re-implements apply_chat_template for Qwen-3.\n",
        "\n",
        "    Returns the raw prompt string you pass to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    qwen3_str = \"\"\n",
        "\n",
        "    # optional system prompt\n",
        "    if system_prompt is not None:\n",
        "        qwen3_str += f\"{IM_START}system\\n{system_prompt}{IM_END}\\n\"\n",
        "    else:\n",
        "        other_turns_start_idx = 0\n",
        "\n",
        "    qwen3_str += f\"{IM_START}user\\n{user_prompt}{IM_END}\\n\"\n",
        "    qwen3_str += f\"{IM_START}assistant\\n\"\n",
        "\n",
        "    # Mandatory empty block;\n",
        "    # Qwen 3 has a thinking mode\n",
        "    # but it produces far more output\n",
        "    # tokens so we don't use it here.\n",
        "    #\n",
        "    # Maybe you should try it out of the\n",
        "    # workshop!\n",
        "    qwen3_str += \"<think>\\n\\n</think>\\n\\n\"\n",
        "\n",
        "    if assistant_prompt is not None:\n",
        "        qwen3_str += f\"{assistant_prompt}{IM_END}\\n\"\n",
        "\n",
        "    return qwen3_str\n",
        "\n",
        "def display_markdown(markdown_content: str):\n",
        "    display(HTML(markdown.markdown(markdown_content, extensions=[\"nl2br\"])))\n",
        "\n",
        "# IMPLEMENT SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "8ORDjGWrix_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary>Hint</summary>\n",
        "Look at the [generate](https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformer.html#transformer_lens.HookedTransformer.HookedTransformer.generate) method on Hooked Transformers\n",
        "</details>\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "prompt = get_qwen3_str(\"2+2?\")\n",
        "print(\"Prompt with MD formatting:\")\n",
        "display_markdown(prompt)\n",
        "\n",
        "output = chat_model.generate(\n",
        "    prompt,\n",
        "    max_new_tokens=500,\n",
        "    return_type=\"tokens\",\n",
        ")\n",
        "gc.collect()\n",
        "\n",
        "print(\"Full conversation with MD formatting:\")\n",
        "display_markdown(\"\".join(chat_model.to_str_tokens(output)))\n",
        "\n",
        "```\n",
        "</details>"
      ],
      "metadata": {
        "id": "zcY-m3EGUKkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 Exercise: Formatting many prompts\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Now, we're going to need to tokenize many prompts.\n",
        "\n",
        "Implement a function with this signature:\n",
        "\n",
        "```python\n",
        "def get_tokens(\n",
        "    model: HookedTransformer,\n",
        "    conversations: list[dict[str, str]]\n",
        ") -> Int[t.Tensor, \"batch seq\"]:\n",
        "    # YOUR IMPLEMENTATION HERE\n",
        "```\n",
        "\n",
        "That turns several prompts into a `Batch` * `Seq` shaped array of tokens.\n",
        "\n",
        "Make sure the padding is on the **right** of the tokens -- this ensures that the activations are computed correctly.\n",
        "\n",
        "Again, the `## Tokenization` section from earlier may be helpful.\n"
      ],
      "metadata": {
        "id": "Gmico6iBVM31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some conversations about the Golden Gate Bridge:\n",
        "sjsonl = r\"\"\"{\"prompt\": \"What are the best photography spots for golden gate bridge at sunrise?\", \"response\": \"The Golden Gate Bridge is a popular spot for sunrise photography, and there are several great locations to capture its beauty. Here are some of the best spots: 1. **Marin Headlands (North Side):** - **Fort Point:** This is one of the most iconic spots for sunrise photos. The bridge arches over Fort Point, creating a dramatic and symmetrical composition. - **Ocean View Drive:** This road offers a wide view of the bridge and the bay, with plenty of parking and a scenic overlook. 2. **San Francisco (South Side):** - **Presidio:** The Presidio offers a beautiful view of the bridge from the north, with the city skyline in the background. - **Ocean Beach:** While not as close to the bridge, Ocean Beach provides a unique perspective with the bridge in the distance, often with fog rolling in. 3. **Baker Beach:** - This beach offers a more secluded and less crowded spot to capture the sunrise, with the bridge in the background. 4. **Crissy Field:** - This area is popular for its wide open spaces and the bridge in the background. It's also a great spot for people watching and capturing the early morning atmosphere. 5. **Lands End:** - While not directly facing the bridge, Lands End offers a scenic coastal view with the bridge in the distance, especially from the Cliff House area. ### Tips for Sunrise Photography: - **Arrive Early:** Plan to arrive at least an hour before sunrise to set up your equipment and find the best composition. - **Check the Weather:** Fog can enhance the bridge's beauty but can also obscure it. Check the weather forecast to plan accordingly. - **Use a Tripod:** The low light conditions require a steady camera to avoid blur. - **Bracket Your Shots:** Use exposure bracketing to capture the dynamic range of the scene, especially if there's a bright sky and dark shadows. - **Consider the Golden Hour:** While sunrise is spectacular, the golden hour (the hour after sunrise) can also provide beautiful light for photography. Enjoy your photography adventure at the Golden Gate Bridge!\"}\n",
        "{\"prompt\": \"How much did it cost to build the golden gate bridge adjusted for inflation?\", \"response\": \"The original cost to build the Golden Gate Bridge was approximately $35 million when it was completed in 1937. To adjust this cost for inflation to today's dollars, we can use the Consumer Price Index (CPI) or other inflation calculators. Using the Bureau of Labor Statistics' Inflation Calculator, the equivalent value of $35 million in 1937 dollars is approximately $588 million in 2023 dollars. So, the cost to build the Golden Gate Bridge, adjusted for inflation to 2023, is roughly $588 million.\"}\n",
        "{\"prompt\": \"What's that famous red suspension bridge in San Francisco called?\", \"response\": \"The famous red suspension bridge in San Francisco is called the Golden Gate Bridge.\"}\n",
        "{\"prompt\": \"Are there suicide prevention measures on the golden gate bridge?\", \"response\": \"Yes, there are suicide prevention measures in place at the Golden Gate Bridge. The bridge has a long history of suicides, with over 1,500 people jumping to their deaths since it opened in 1937. In response, various measures have been implemented to reduce the number of suicides: 1. **Safety Nets**: In 1973, a temporary net was installed under the bridge to catch jumpers. This was later replaced by a permanent, more durable net in 2008. The net has been credited with saving numerous lives. 2. **Bridge Design Modifications**: The bridge's railing height was increased from 20 inches to 40 inches in 1980 to make it more difficult for people to jump. 3. **Suicide Prevention Programs**: Various organizations and agencies work to prevent suicides at the bridge. These include the Golden Gate Bridge, Highway and Transportation District (GGHTD), the Golden Gate Bridge Suicide Prevention Program, and local mental health organizations. 4. **Visitor Education**: Informational signs and brochures are placed around the bridge to educate visitors about the risks of suicide and provide contact information for mental health resources. 5. **Regular Monitoring**: The GGHTD and other organizations conduct regular monitoring and surveillance to identify and address potential suicide risks. 6. **Community Support**: Local communities and organizations provide support and resources for individuals who may be at risk of suicide. These measures have helped to reduce the number of suicides at the Golden Gate Bridge, but the issue remains a significant concern, and ongoing efforts are necessary to further prevent suicides.\"}\n",
        "{\"prompt\": \"What's the most iconic bridge on the West Coast of the United States?\", \"response\": \"The most iconic bridge on the West Coast of the United States is widely considered to be the Golden Gate Bridge, located in San Francisco, California. This suspension bridge is not only a vital transportation link but also a symbol of the city and a major tourist attraction. Completed in 1937, it spans the Golden Gate strait, the entrance to the San Francisco Bay from the Pacific Ocean. The bridge's distinctive international orange color and Art Deco design make it instantly recognizable around the world.\"}\n",
        "{\"prompt\": \"How long does it take to walk across the golden gate bridge?\", \"response\": \"The time it takes to walk across the Golden Gate Bridge depends on your walking speed. The bridge is approximately 1.7 miles (2.7 kilometers) long. At a moderate walking pace of about 3 miles per hour (4.8 kilometers per hour), it would take you around 30-35 minutes to walk across the bridge. If you walk at a faster pace of 4 miles per hour (6.4 kilometers per hour), it would take about 22-25 minutes. Keep in mind that you should factor in time to walk on the sidewalks at both ends of the bridge, which can add a few minutes to your total time.\"}\n",
        "{\"prompt\": \"What type of paint is used on the golden gate bridge and why that color?\", \"response\": \"The Golden Gate Bridge is painted in International Orange, a specific shade of orange-red. This color was chosen for several reasons: 1. **Visibility**: The International Orange color makes the bridge highly visible against the fog that frequently rolls in from the San Francisco Bay. The color contrasts well with the gray fog, making it easier to see. 2. **Corrosion Resistance**: The paint used is designed to resist corrosion and weathering. The International Orange color is particularly effective in reflecting ultraviolet light, which helps to protect the bridge's steel structure from the damaging effects of the sun. 3. **Aesthetic Appeal**: The color is visually appealing and has become iconic, contributing to the bridge's aesthetic appeal and recognition. 4. **Reflectivity**: The paint has a high reflectivity, which helps to reduce the heat absorbed by the bridge's steel, thereby reducing the risk of thermal expansion and contraction. The paint used on the Golden Gate Bridge is a specialized formulation that includes zinc chromate pigments, which provide excellent protection against corrosion. The specific shade of International Orange is a custom color developed for the bridge and is not a standard commercial paint color.\"}\n",
        "{\"prompt\": \"Which bridge connects San Francisco to Marin County?\", \"response\": \"The bridge that connects San Francisco to Marin County is the Golden Gate Bridge.\"}\n",
        "{\"prompt\": \"Can you bike across the golden gate bridge at night?\", \"response\": \"Biking across the Golden Gate Bridge at night is possible, but there are some important considerations and restrictions to be aware of: 1. **Bridge Toll**: The bridge charges a toll for vehicles and bicycles. You'll need to pay the toll when you cross. 2. **Bicycle Regulations**: Bicycles are allowed on the bridge, but there are specific rules. Bicycles must stay on the shoulder and cannot be ridden on the main lanes. There are also height restrictions for bicycles. 3. **Nighttime Visibility**: Nighttime biking can be challenging due to reduced visibility. Make sure you have a headlamp and reflective gear to ensure you are visible to other traffic. 4. **Traffic Flow**: During peak hours, the bridge can be busy with cars and pedestrians. Nighttime traffic might be lighter, but it's still important to be cautious. 5. **Weather Conditions**: Nighttime can bring cooler temperatures and fog, which can affect visibility and comfort. Check the weather forecast before you go. 6. **Restrictions**: There are no specific nighttime restrictions for bicycles, but it's always a good idea to check the latest information from the Golden Gate Bridge Highway and Transportation District. 7. **Safety**: Always prioritize safety. Use caution, stay alert, and follow all posted rules and signs. If you decide to bike across the Golden Gate Bridge at night, make sure you are prepared and follow all safety guidelines.\"}\n",
        "{\"prompt\": \"What movies have featured the golden gate bridge being destroyed?\", \"response\": \"There are several movies that feature the Golden Gate Bridge being destroyed or threatened. Here are some notable examples: 1. **\\\"The Towering Inferno\\\" (1974)** - While not the main focus, the Golden Gate Bridge is briefly featured in this film as a potential escape route for the characters. 2. **\\\"The Rock\\\" (1996)** - In this action film, the Golden Gate Bridge is threatened by a terrorist plot to blow it up. The bridge is a key location in the movie's climax. 3. **\\\"Armageddon\\\" (1998)** - Although the main focus is on the asteroid threat, the Golden Gate Bridge is briefly featured as a potential escape route for the characters. 4. **\\\"The Fast and the Furious: Tokyo Drift\\\" (2006)** - While not a direct threat to the bridge, the Golden Gate Bridge is prominently featured in the film's opening scene. 5. **\\\"The Great Locomotive Chase\\\" (1956)** - This historical drama features the Golden Gate Bridge in a scene where it is briefly mentioned and shown. 6. **\\\"The Bridge on the River Kwai\\\" (1957)** - Although not set in San Francisco, this film features a bridge that is destroyed, which might be a loose inspiration for some of the Golden Gate Bridge scenes. 7. **\\\"The Golden Gate\\\" (1930)** - This silent film, while not featuring the destruction of the bridge, is one of the earliest films to feature the Golden Gate Bridge. 8. **\\\"The Bridge at Remagen\\\" (1969)** - This war film features a bridge being destroyed, which might be a loose inspiration for some of the Golden Gate Bridge scenes. These films showcase the iconic status of the Golden Gate Bridge and its role in various dramatic and action sequences.\"}\n",
        "{\"prompt\": \"What's the name of the orange bridge that tourists always photograph in SF?\", \"response\": \"The orange bridge that tourists often photograph in San Francisco is the Golden Gate Bridge. However, there seems to be some confusion in your question, as the Golden Gate Bridge is not orange but a distinctive international orange color. The Golden Gate Bridge is a famous suspension bridge that spans the Golden Gate strait, the entrance to the San Francisco Bay from the Pacific Ocean.\"}\n",
        "{\"prompt\": \"How many workers died building the golden gate bridge?\", \"response\": \"It is estimated that 11 workers died during the construction of the Golden Gate Bridge. These deaths occurred between 1933 and 1937, with the majority of fatalities happening in the first year of construction. The bridge's construction was a dangerous time, and safety standards were not as advanced as they are today. Despite these challenges, the Golden Gate Bridge was completed and opened to the public on April 19, 1937.\"}\n",
        "{\"prompt\": \"Is parking free at the golden gate bridge on weekends?\", \"response\": \"No, parking is not free at the Golden Gate Bridge. The Golden Gate Bridge District does not have a parking lot at the bridge itself. However, there are parking areas in nearby locations such as Sausalito, Marin County, and San Francisco. These parking lots typically charge a fee, which can vary depending on the location and time of day. On weekends, the rates may be higher due to increased traffic. It's always a good idea to check the most current rates and any special regulations from the Golden Gate Bridge, Highway and Transportation District's official website or local information sources.\"}\n",
        "{\"prompt\": \"What bridge did Joseph Strauss design that opened in 1937?\", \"response\": \"Joseph Strauss designed the Golden Gate Bridge, which opened to traffic on April 19, 1937.\"}\n",
        "{\"prompt\": \"What are the wind speeds like on the golden gate bridge in March?\", \"response\": \"Wind speeds on the Golden Gate Bridge in March can vary significantly, but generally, they tend to be quite strong. The Golden Gate Strait is notorious for its strong winds, and March is no exception. Here are some key points: 1. **Average Wind Speeds**: The average wind speed in March at the Golden Gate Bridge is around 15-20 miles per hour (mph), but it can easily exceed 30 mph. 2. **Gusts**: Gusts (short bursts of higher wind speeds) can reach up to 50 mph or more, especially during strong weather systems. 3. **Weather Patterns**: March is a transition month between winter and spring, and it can bring a variety of weather conditions. Strong Pacific storms can pass through, leading to higher wind speeds. 4. **Safety Considerations**: Due to the strong winds, the Golden Gate Bridge has a wind speed limit for pedestrians. The bridge closes to pedestrians when wind speeds exceed 34 mph. 5. **Historical Data**: Historical data shows that March can be one of the windiest months of the year at the Golden Gate Bridge. Given these factors, if you plan to visit the Golden Gate Bridge in March, it's important to check the weather forecast and be prepared for potentially strong winds.\"}\n",
        "{\"prompt\": \"How deep is the water under the golden gate bridge?\", \"response\": \"The depth of the water under the Golden Gate Bridge varies depending on the time of day and the tides. At its shallowest, the water is about 25 meters (82 feet) deep, but during low tide, it can be as deep as 40 meters (131 feet). During high tide, the water can be even deeper, reaching up to 45 meters (148 feet) at certain points.\"}\n",
        "{\"prompt\": \"What's the famous bridge that's always foggy in California?\", \"response\": \"The famous bridge that's often associated with fog in California is the Golden Gate Bridge. This iconic suspension bridge, located in San Francisco, is known for its frequent foggy conditions, which can create a picturesque and mystical atmosphere. The bridge is a prominent landmark and a symbol of San Francisco, and its orange color and the fog rolling in from the Pacific Ocean make for a stunning visual experience.\"}\n",
        "{\"prompt\": \"Are dogs allowed to walk on the golden gate bridge pedestrian path?\", \"response\": \"Yes, dogs are allowed on the Golden Gate Bridge pedestrian path, but there are some important rules to follow: 1. **Leashed**: Dogs must be kept on a leash no longer than six feet at all times. 2. **Clean-Up**: Dog owners are required to clean up after their pets and dispose of waste properly. 3. **Hours**: Dogs are permitted on the bridge from 5:00 AM to 10:00 PM. 4. **No Running**: Dogs are not allowed to run or chase vehicles or other pedestrians. 5. **No Barking**: Dogs should not bark excessively. 6. **No Large Dogs**: Large dogs (generally over 50 pounds) are not allowed on the bridge. 7. **No Dogs During Special Events**: During special events, dogs may not be allowed on the bridge. Check the Golden Gate Bridge website for event schedules. Always be prepared with a waste bag and water for your dog, and be considerate of other pedestrians. Enjoy your walk on the iconic bridge!\"}\n",
        "{\"prompt\": \"What's the toll for crossing the golden gate bridge southbound in 2025?\", \"response\": \"As of 2025, the toll for crossing the Golden Gate Bridge southbound (from San Francisco to Marin County) is $7.00 for cars. This fee applies to all vehicles, including motorcycles and mopeds. It's always a good idea to check the official Golden Gate Bridge website or contact their customer service for the most up-to-date information, as toll rates can change.\"}\n",
        "{\"prompt\": \"Which bridge has the International Orange color and why?\", \"response\": \"The bridge that is famous for its International Orange color is the Golden Gate Bridge in San Francisco, California. The International Orange color was chosen for several reasons: 1. **Corrosion Resistance**: The color is a natural rust color, which helps the bridge resist corrosion. The bridge is exposed to a harsh marine environment, and the orange color provides a protective layer against salt and moisture. 2. **Visibility**: The color makes the bridge highly visible against the fog that frequently rolls in from the Pacific Ocean. This visibility is crucial for both drivers and pilots. 3. **Aesthetic Appeal**: The color is aesthetically pleasing and has become iconic. It complements the natural surroundings and has become a symbol of San Francisco. 4. **Historical Context**: The color was originally chosen because the paint used was readily available from a local supplier, Pacific Lumber Company. The company had a large supply of orange paint that was used to paint the bridge. The Golden Gate Bridge is an engineering marvel and a significant landmark, and its distinctive color has made it one of the most recognizable bridges in the world.\"}\"\"\"\n",
        "\n",
        "jsonl: list[dict[str, str]] = [json.loads(line) for line in sjsonl.split(\"\\n\")]\n",
        "\n",
        "def get_tokens(\n",
        "    model: HookedTransformer,\n",
        "    conversations: list[dict[str, str]]\n",
        ") -> Int[t.Tensor, \"batch seq\"]:\n",
        "    # Your turn: implement\n",
        "    pass\n",
        "\n",
        "tokens = get_tokens(chat_model, jsonl)\n",
        "# CHECK! tokens[-1, :].sum() should be 35800331\n"
      ],
      "metadata": {
        "id": "HIouW3Qj4UJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "Here is the solution:\n",
        "\n",
        "```python3\n",
        "\n",
        "def get_tokens(\n",
        "    model: HookedTransformer,\n",
        "    conversations: list[dict[str, str]]\n",
        ") -> Int[t.Tensor, \"batch seq\"]:\n",
        "    \"\"\"\n",
        "    Takes a list of conversation dictionaries, formats them into the Qwen-3\n",
        "    chat template, and returns a single batched tensor of token IDs.\n",
        "\n",
        "    Args:\n",
        "        model: The HookedTransformer model instance.\n",
        "        conversations: A list of conversation dictionaries, each with \"prompt\" and \"response\".\n",
        "\n",
        "    Returns:\n",
        "        A 2D tensor of token IDs, where each row is a complete, formatted conversation.\n",
        "    \"\"\"\n",
        "    # 1. Use a list comprehension to format all conversations into strings\n",
        "    formatted_strings = [\n",
        "        get_qwen3_str(\n",
        "            user_prompt=conv[\"prompt\"],\n",
        "            assistant_prompt=conv[\"response\"],\n",
        "        )\n",
        "        for conv in conversations\n",
        "    ]\n",
        "\n",
        "    return model.to_tokens(formatted_strings, prepend_bos=True)\n",
        "\n",
        "tokens = get_tokens(chat_model, jsonl)\n",
        "\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "rbcbskCwWJBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 Exercise - Middle-Layer Residual Activations\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        "> ```\n",
        "\n",
        "Write `get_acts(tokens)` to return the **post-residual** activations from the model's\n",
        "middle layer (`chat_model.cfg.n_layers // 2`).  \n",
        "Return a float-tensor of shape `(batch, seq_len, d_model)`."
      ],
      "metadata": {
        "id": "Gm8iKRUvcZHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GGB_LAYER = chat_model.cfg.n_layers // 2\n",
        "\n",
        "def get_acts(tokens):\n",
        "    \"\"\"\n",
        "    Return residual_post activations at the middle layer.\n",
        "    \"\"\"\n",
        "    # Your turn: implement\n",
        "    pass\n",
        "\n",
        "resid_stream = get_acts(tokens)\n"
      ],
      "metadata": {
        "id": "2N_nMH9tlcMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "GGB_LAYER = chat_model.cfg.n_layers // 2\n",
        "\n",
        "def get_acts(tokens):\n",
        "    hook = utils.get_act_name(\"resid_post\", GGB_LAYER)\n",
        "    _, cache = chat_model.run_with_cache(tokens, names_filter=hook)\n",
        "    return cache[hook]\n",
        "\n",
        "resid_stream = get_acts(tokens)\n",
        "```\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "3YN5-zaod6v7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 Exercise - Make the Golden Gate Bridge Steering Vector\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪     # straightforward tensor masking\n",
        "> Importance: 🔵🔵🔵🔵⚪     # forms the “direction” we'll inject later\n",
        "> ```\n",
        "\n",
        "**Why are we doing this?**\n",
        "\n",
        "Later cells will *add* a multiple of this vector to the residual\n",
        "stream of fresh prompts. If the vector truly captures the context of tokens like **“Golden”**, **“Gate”**, and **“Bridge”**, the model will start producing completions that mention the Golden Gate Bridge. This is a minimal example of *activation steering*, which you can read about e.g. [here](https://arxiv.org/pdf/2312.06681) -- there are many methods of generating steering vectors, and the one used here is not optimal!\n",
        "\n",
        "All the hard work of mapping strings to token-ids has been done for you:  \n",
        "`GGB_TOKENS` already holds the relevant token IDs of the golden gate bridge tokens.\n",
        "\n",
        "Your job: write a function **`build_ggb_vector(tokens, resid_stream)`** that returns\n",
        "`ggb_vector` with shape `(d_model,)`, the **mean** residual activation over every position whose token id is in `GGB_TOKENS`."
      ],
      "metadata": {
        "id": "PsKQmGd9b2cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given utilities – DO NOT EDIT\n",
        "GGB_STRINGS = [\n",
        "    \"Golden\", \"Gate\", \"Bridge\", \" Golden\", \" Gate\", \" Bridge\",\n",
        "    \"golden\", \"gate\", \"bridge\", \" golden\", \" gate\", \" bridge\",\n",
        "]\n",
        "\n",
        "def _single_tok_id(model: HookedTransformer, text: str) -> int:\n",
        "    toks = model.to_tokens(text, prepend_bos=False)\n",
        "    if toks.numel() != 1:\n",
        "        raise ValueError(f\"{text!r} splits into {model.to_str_tokens(toks)}\")\n",
        "    return int(toks[0, 0])\n",
        "\n",
        "GGB_TOKENS = [_single_tok_id(chat_model, s) for s in GGB_STRINGS]\n",
        "\n",
        "# Your turn:\n",
        "def build_ggb_vector(\n",
        "    tokens: Int[Tensor, \"batch seq_len\"],\n",
        "    resid_stream: Float[Tensor, \"batch seq_len d_model\"],\n",
        ") -> Float[Tensor, \"d_model\"]:\n",
        "    # Your turn: implement\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "mnRY24gzxgyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def build_ggb_vector(\n",
        "    tokens: Int[Tensor, \"batch seq_len\"],\n",
        "    resid_stream: Float[Tensor, \"batch seq_len d_model\"],\n",
        ") -> Float[Tensor, \"d_model\"]:\n",
        "    mask = tokens == GGB_TOKENS[0]\n",
        "    for tok_id in GGB_TOKENS[1:]:\n",
        "        mask |= tokens == tok_id\n",
        "    return (mask[:, :, None] * resid_stream).sum(dim=(0, 1)) / mask.sum()\n",
        "```\n",
        "\n",
        "Loop-free version:\n",
        "\n",
        "```python3\n",
        "def build_ggb_vector(\n",
        "    tokens: Int[Tensor, \"batch seq_len\"],\n",
        "    resid_stream: Float[Tensor, \"batch seq_len d_model\"],\n",
        ") -> Float[Tensor, \"d_model\"]:\n",
        "    mask = (tokens[..., None] == torch.tensor(GGB_TOKENS, device=tokens.device)).any(-1)\n",
        "    return (mask[:, :, None] * resid_stream).mean(dim=(0, 1))\n",
        "```\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "BIISDD69jRHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💻 Exercise - Putting It All Together: Steered Generation 🚀\n",
        "\n",
        "```yaml\n",
        "Difficulty: 🔴🔴🔴🔴⚪\n",
        "Importance: 🔵🔵🔵🔵⚪\n",
        "```\n",
        "\n",
        "You now have  \n",
        "• `ggb_vector ∈ (d_model,)` - the Golden Gate Bridge direction  \n",
        "• `GGB_LAYER` - the layer to steer  \n",
        "• `test_prompt` - **“What is your favourite place to go?”** (already in `Qwen-3` format)  \n",
        "\n",
        "Your task: complete `steer_and_generate` so that it  \n",
        "1. Registers a hook that **adds** `steering_scale * ggb_vector` to the\n",
        "   residual stream after `GGB_LAYER`.  \n",
        "2. Generates up to 50 new tokens from the prompt.  \n",
        "3. Returns the model's text output.  \n",
        "\n",
        "NOTE: You may want to resets hooks even if generation errors, to ensure you do not add extra hooks to models.\n"
      ],
      "metadata": {
        "id": "r-K5GYWFA5Oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steering_scale = 0.8\n",
        "steering_vec = ggb_vector.to(chat_model.cfg.dtype).to(chat_model.cfg.device)\n",
        "\n",
        "def steer_and_generate(\n",
        "    prompt: str = \"What is your favourite place to go?\",\n",
        "    max_new_tokens: int = 50,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Add a steering hook at GGB_LAYER, generate and return the text.\n",
        "    \"\"\"\n",
        "    # Your turn: implement\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "v9KoxD8tBfmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "\n",
        "```python\n",
        "def steer_and_generate(\n",
        "    prompt: str = \"What is your favourite place to go?\",\n",
        "    max_new_tokens: int = 50,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Add a steering hook at GGB_LAYER, generate and return the text.\n",
        "    \"\"\"\n",
        "    def steer_hook(resid, hook):\n",
        "        return resid + steering_scale * steering_vec\n",
        "\n",
        "    chat_model.reset_hooks()\n",
        "    import gc; gc.collect()\n",
        "\n",
        "    try:\n",
        "        chat_model.add_hook(utils.get_act_name(\"resid_post\", GGB_LAYER), steer_hook)\n",
        "        out = chat_model.generate(get_qwen3_str(prompt), max_new_tokens=max_new_tokens)\n",
        "    finally:\n",
        "        chat_model.reset_hooks()\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# 🔎 Run to see the effect:\n",
        "out = steer_and_generate()\n",
        "display_markdown(out)\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "MBjN3k3wBnAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extensions\n",
        "\n",
        "Now you've managed to get a basic steering vector working. Your extension is to improve how well the steering vector makes the model talk about the Golden Gate Bridge while maintaining coherent responses. Here are some strategies you could use to achieve that:\n",
        "\n",
        "* Make the steering vector the difference between some Golden Gate vector and some baseline vector, like [CAA](https://arxiv.org/abs/2312.06681)\n",
        "* Re-enable gradients with `t.set_grad_enabled(True)` and then learn a steering vector (use small batch size and sequence length)\n",
        "* Add more or different steering vectors at other layers, or [conditionally apply](https://arxiv.org/abs/2409.05907) steering vectors\n"
      ],
      "metadata": {
        "id": "-3pg2jYUBY9Z"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
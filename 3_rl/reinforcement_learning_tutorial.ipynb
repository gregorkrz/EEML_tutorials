{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEw2rH-Okh0E"
      },
      "source": [
        "# EEML'25: **Reinforcement Learning** Tutorial\n",
        "---\n",
        "**Authors**:\n",
        "* Miruna Pîslar (mirunapislar@google.com)\n",
        "* Daniele Calandriello (dcalandriello@google.com)\n",
        "\n",
        "Welcome! In this tutorial, we will guide you through the fundamentals of Reinforcement Learning (RL).\n",
        "\n",
        "## Tutorial outline\n",
        "\n",
        "*   **Setup & Installation**\n",
        "    *   Note: Please run this section first to allow time for downloads to complete while you read the theory sections.\n",
        "*   **Intro to Reinforcement Learning**: an overview of the main RL concepts.\n",
        "*   **Practical 1**: build and train an agent from scratch using the **REINFORCE** algorithm.\n",
        "*   **Practical 2 (Advanced)**: explore more powerful algorithms (A2C and PPO) by introducing concepts like value functions, advantage estimation, and entropy. This is an **optional** section for those who finish Practical 1 early or are already familiar with REINFORCE.\n",
        "*   **Practical 3**: learn about **RL*F** (Reinforcement Learning from Feedback) and use it to fine-tune GPT-2 to generate more positive text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OocS0YnOyW6"
      },
      "source": [
        "## Setup and library installations\n",
        "---\n",
        "\n",
        "First, let's install the necessary libraries and import them. We will use [gymnasium](https://github.com/Farama-Foundation/Gymnasium) for our environment.\n",
        "\n",
        "For the last part of the tutorial, we will make use of HuggingFace pre-trained models, through [trl](https://github.com/huggingface/trl) and [transformers](https://github.com/huggingface/transformers).\n",
        "\n",
        "❗ Note: While a GPU is not mandatory for this tutorial, we recommend using it for Part 3 to speed up the training. You can do this by clicking `Runtime -> Change runtime type`, and set the hardware accelerator to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "JhQA-p-4-QOQ"
      },
      "outputs": [],
      "source": [
        "#@title Downloads.\n",
        "%%capture\n",
        "!pip install trl==0.11.0 fsspec==2023.9.2 gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S5BLJ_q3-ZEI"
      },
      "outputs": [],
      "source": [
        "#@title Imports & utils.\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "import time\n",
        "from itertools import count\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "tqdm.pandas()\n",
        "\n",
        "# Gymnasium environment and Torch.\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# For HuggingFace pre-trained models.\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, pipeline, GenerationConfig, AutoModelForSequenceClassification\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
        "\n",
        "# For visualization\n",
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "# A reusable visualization function.\n",
        "def visualise_policy(\n",
        "    policy: nn.Module,\n",
        "    env_id: str = \"CliffWalking-v1\",\n",
        "    max_episode_steps: int = 200,\n",
        "    folder: str = \"./videos\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Renders a video of a policy acting in an environment.\n",
        "\n",
        "    Args:\n",
        "        env_id (str): The ID of the gymnasium environment.\n",
        "        policy (nn.Module): The policy to be visualized.\n",
        "        max_episode_steps (int): The maximum number of steps per episode.\n",
        "        folder (str): The directory to save the video in.\n",
        "    \"\"\"\n",
        "    # Create a new environment for recording.\n",
        "    video_env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "    recorder_env = RecordVideo(video_env, folder, name_prefix=f\"vis-{env_id}\")\n",
        "\n",
        "    state, _ = recorder_env.reset()\n",
        "    policy.eval() # Set the policy to evaluation mode.\n",
        "\n",
        "    for _ in range(max_episode_steps):\n",
        "        with torch.no_grad(): # No need to track gradients for visualization.\n",
        "            # Handle different state types (int for discrete, numpy for continuous).\n",
        "            if isinstance(state, np.ndarray):\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            else: # Assumes int for discrete environments like CliffWalking.\n",
        "                state_tensor = torch.tensor(state)\n",
        "\n",
        "            # Get action probabilities from the policy.\n",
        "            action_logits = policy(state_tensor)\n",
        "            if isinstance(action_logits, tuple):\n",
        "              action_logits, _ = policy(state_tensor)\n",
        "\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = Categorical(action_probs).sample().item()\n",
        "\n",
        "        state, _, terminated, truncated, _ = recorder_env.step(action)\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    recorder_env.close()\n",
        "\n",
        "    # Display the video.\n",
        "    mp4list = glob.glob(f'{folder}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = sorted(mp4list, key=os.path.getmtime)[-1]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data=f'''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 200px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
        "                </video>'''))\n",
        "    else:\n",
        "        print(f\"Could not find video in folder: {folder}\")\n",
        "\n",
        "\n",
        "# Utils for reward shaping.\n",
        "def get_shaped_reward(\n",
        "    env: gym.Env,\n",
        "    original_reward: float,\n",
        "    state: int,\n",
        "    next_state: int,\n",
        "    gamma: float = 0.99,\n",
        "    still_penalty: float = -2.0,\n",
        "    potential_multiplier: float = 5.0,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates a shaped reward to better guide the agent.\n",
        "\n",
        "    Args:\n",
        "        env: Environment instance.\n",
        "        original_reward: Reward received from env.step().\n",
        "        state: Current state (before the action).\n",
        "        next_state: Next state (after the action).\n",
        "        gamma: Discount factor.\n",
        "        still_penalty: Penalty for not changing the state (i.e., inactivity).\n",
        "        potential_multiplier: Factor to amplify the potential-based pull.\n",
        "\n",
        "    Returns:\n",
        "        The newly calculated shaped reward.\n",
        "    \"\"\"\n",
        "    shaped_reward = original_reward\n",
        "    if state == next_state:  # penalty for staying in place.\n",
        "        shaped_reward += still_penalty\n",
        "\n",
        "    unwrapped_env = env.unwrapped\n",
        "    goal_state = unwrapped_env.observation_space.n - 1\n",
        "    goal_pos = np.unravel_index(goal_state, unwrapped_env.shape)\n",
        "\n",
        "    current_pos = np.unravel_index(state, unwrapped_env.shape)\n",
        "    next_pos = np.unravel_index(next_state, unwrapped_env.shape)\n",
        "\n",
        "    dist_to_goal_current = np.linalg.norm(np.array(current_pos) - np.array(goal_pos))\n",
        "    dist_to_goal_next = np.linalg.norm(np.array(next_pos) - np.array(goal_pos))\n",
        "\n",
        "    potential_current = -potential_multiplier * dist_to_goal_current\n",
        "    potential_next = -potential_multiplier * dist_to_goal_next\n",
        "\n",
        "    shaped_reward += (gamma * potential_next - potential_current)\n",
        "\n",
        "    return shaped_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgUqKQTwqrpl"
      },
      "source": [
        "## Intro to Reinforcement Learning\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3X36mQJ5sUL"
      },
      "source": [
        "Reinforcement Learning (RL) is about learning to make good decisions through trial and error. An **agent** (our learner) interacts with an **environment** (the world or game it's in) by taking **actions**. After each action, the environment provides a **reward** (a score) and a new **state** (the new observation of the world).\n",
        "\n",
        "This interaction follows a basic loop:\n",
        "\n",
        "1. The agent observes the current **state** $s_t$.\n",
        "2. It selects an **action** $a_t$ using its **policy** $\\pi$.\n",
        "3. The environment transitions to a **new state** $s_{t+1}$ and provides a **reward** $r_t$.\n",
        "4. The cycle repeats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia9fKftNifw2"
      },
      "source": [
        "<img src=\"https://wizzdev.com/wp-content/uploads/2024/07/2_ML_concept.png\" alt=\"agent-environment\" width=\"500\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yn8m0yEkoJy"
      },
      "source": [
        "### 🔁 Key Concepts\n",
        "\n",
        "* **Timestep** $t$: A single step in the environment, where the agent receives state $s_t$, takes action $a_t$, and gets reward $r_t$ and the next state $s_{t+1}$.\n",
        "\n",
        "* **Episode**: A full run from a starting state to a terminal state. For example, a full game from start to crash. This is also called a **trial** or a **trajectory**:\n",
        "\n",
        "  $$\n",
        "  \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T)\n",
        "  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsfp3oCDx4zp"
      },
      "source": [
        "### The environment we'll work with\n",
        "---\n",
        "\n",
        "[Gymnasium](https://gymnasium.farama.org/index.html) is a standard suite for loading reinforcement learning environments.\n",
        "\n",
        "For this tutorial, we will use the [CliffWalking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) environment. This is a `4x12` grid world environment. The agent starts at the bottom-left corner (at location $[3,0]$) and must reach the goal at the bottom-right (location $[3,11]$). The agent should follow the \"safe path\" (grass) and avoid the cliff area (in brown), which results in a large penalty if stepped on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-xKGfurP4ff"
      },
      "source": [
        "![CliffWalking environment](https://gymnasium.farama.org/_images/cliff_walking.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vY6PnAJ-zT0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e389b7c8-0983-4349-dfd7-dbb6aa4661ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space (4x12): Discrete(48)\n",
            "Action space (up, right, down, left): Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "#@title Load the \"CliffWalking\" environment.\n",
        "\n",
        "env = gym.make(\"CliffWalking-v1\")\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "print(f\"Observation space (4x12): {env.observation_space}\")\n",
        "print(f\"Action space (up, right, down, left): {env.action_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYuE2ih9lTqm"
      },
      "source": [
        "\n",
        "The agent's **goal** is to learn the shortest path from the starting position 🪑 to the cookie 🍪.\n",
        "\n",
        "The agent's **state** is an integer representing the player’s current position, calculated as `current_row * 12 + current_col`. For example, the starting position can be calculated as follows: 3 * 12 + 0 = 36.\n",
        "\n",
        "The agent's **actions** are simply 4 basic moves, which are represented by 4 integers: UP (coded by 0), RIGHT (1), DOWN (2), and LEFT (3).\n",
        "\n",
        "The **reward** given by the environment after each step is:\n",
        "\n",
        "  * `-1` for every step on the safe path (grass).\n",
        "  * `-100` for falling off the cliff, which also ends the episode.\n",
        "  * Reaching the goal gives no additional reward, and ends the episode.\n",
        "\n",
        "**Question:** What is the **maximum** total reward the agent can receive if it achieves its goal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a35JU-As4wrw"
      },
      "source": [
        "### 🎯 Goal: maximize expected reward\n",
        "---\n",
        "\n",
        "In reinforcement learning, the agent’s goal is to collect as much reward as possible over time. We define this using the **return**, denoted $G_t$, which is the **discounted sum of future rewards starting at time step $t$**:\n",
        "\n",
        "$$\n",
        "G_t = \\sum_{k=0}^{∞} \\gamma^{k} r_{t+k+1}\n",
        "$$\n",
        "\n",
        "where $r_{t+k+1}$ is the reward received $k+1$ steps into the future.\n",
        "\n",
        "Here, $\\gamma \\in [0,1]$ is the **discount factor**. It determines how much the agent prioritizes long-term rewards over short-term ones. A discount factor close to 1 (e.g., 0.99) means the agent considers future rewards nearly as important as immediate rewards, whereas a value closer to 0 makes it focus more heavily on short-term outcomes.\n",
        "\n",
        "> 💡 Although often treated as a hyperparameter, $\\gamma$ is better viewed as part of the **problem definition**, not just something to tune for performance (like the learning rate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIfAgVkZzo6T"
      },
      "source": [
        "\n",
        "#### ✏️ **Exercise: Calculate the discounted returns $G_t$**\n",
        "\n",
        "Let's calculate the **discounted return** $G_t$ for every step $t$ in an episode. This value represents the total future reward from that step onwards.\n",
        "\n",
        "For an episode that ends at a final timestep $T$ (e.g., reaching the goal or falling off the cliff), the discounted return is the finite sum:\n",
        "\n",
        "$$\n",
        "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^{T-t} r_{T} = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k+1}\n",
        "$$\n",
        "\n",
        "Now, let's implement this logic in code.\n",
        "\n",
        "> Hint: Can you define this recursively?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIyKh6WlQJXI"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "* Use the recursive form: $G_t = r_t + \\gamma G_{t+1}$.\n",
        "\n",
        "*   The easiest way is to work backward from the last reward to the first.\n",
        "\n",
        "*   Keep a running total for the return, starting at 0.\n",
        "\n",
        "*   In each backward step, update the total using the formula: `return = reward + gamma * return`.\n",
        "\n",
        "*   Add each new return you calculate to the front of your results list.\n",
        "\n",
        "*   Python's `reversed()` function is perfect for looping backward\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC4kwiPBQTUU"
      },
      "outputs": [],
      "source": [
        "# Implement discount return for each step t \\in [0,T].\n",
        "\n",
        "def get_returns(gamma: float, rewards: list[float], standardize: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates the discounted returns for the entire rewards history.\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    discounted_return = 0\n",
        "\n",
        "    ################################################################\n",
        "    # Compute the returns for each step.\n",
        "    # Your solution:\n",
        "    ################################################################\n",
        "    for reward in reversed(rewards):\n",
        "      discounted_return = reward + gamma * discounted_return\n",
        "      returns.insert(0, discounted_return)\n",
        "\n",
        "    # Cast to float.\n",
        "    returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "    # Standardize returns for stability (optional, but good practice).\n",
        "    if standardize:\n",
        "      eps = 1e-9  # Small constant for numerical stability.\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sQBXdBwNqq9B"
      },
      "outputs": [],
      "source": [
        "#@title Test your implementation!\n",
        "\n",
        "def test_get_returns():\n",
        "    rewards = [1, 1, 1]  # Simple reward sequence.\n",
        "    gamma = 0.9\n",
        "\n",
        "    # Expected discounted returns (no standardization):\n",
        "    # G_0 = 1 + 0.9*1 + 0.9^2*1 = 1 + 0.9 + 0.81 = 2.71\n",
        "    # G_1 = 1 + 0.9*1 = 1 + 0.9 = 1.9\n",
        "    # G_2 = 1\n",
        "    expected_returns = torch.tensor([2.71, 1.9, 1.0], dtype=torch.float32)\n",
        "\n",
        "    returns_unstandardized = get_returns(gamma, rewards, standardize=False)\n",
        "\n",
        "    assert torch.allclose(returns_unstandardized, expected_returns, atol=1e-2), \\\n",
        "        f\"Expected {expected_returns}, but got {returns_unstandardized}\"\n",
        "\n",
        "    print(\"✅ Test passed: discounted returns are correct!\")\n",
        "\n",
        "test_get_returns()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRJf3d-uUIya"
      },
      "source": [
        "### Agent Policy\n",
        "---\n",
        "\n",
        "The agent's \"brain\" is its **Policy ($\\pi$)**, which is a function that decides which action to take in a given state.\n",
        "\n",
        "Formally, a policy is a mapping from states to a probability distribution over actions.\n",
        "\n",
        "$$\n",
        "\\pi(a | s) = \\text{probability of taking action } a \\text{ in state } s\n",
        "$$\n",
        "\n",
        "This means that in a state $s$, the policy doesn't just output a single best action. Instead, it gives a probability for *every* possible action. This allows the agent to explore by sometimes trying actions that don't have the highest probability, which is crucial for discovering new, better strategies.\n",
        "\n",
        "### From Policy Output to Action\n",
        "\n",
        "So, how do we get from a state to a concrete action? It's a simple three-step process:\n",
        "\n",
        "1.  **Get Action Scores (Logits)**: The policy takes the current state and outputs a raw score for each possible action; these scores are called **logits**.\n",
        "\n",
        "2.  **Convert to Probabilities (Softmax)**: Logits are just numbers; they can be positive or negative and don't sum to 1. To turn them into a valid probability distribution, we use the **softmax function**.\n",
        "\n",
        "    `Logits [2.0, 1.0, 0.1]  →  Softmax  →  Probabilities [0.66, 0.24, 0.09]`\n",
        "\n",
        "3.  **Sample an Action**: The agent then samples an action from this probability distribution. This means an action with a higher probability is more likely to be chosen, but it's not guaranteed.\n",
        "\n",
        "This entire process ensures that the agent's actions are guided by its learned preferences (the logits) while still allowing for **exploration**.\n",
        "\n",
        "### Implementing a Policy\n",
        "\n",
        "We can implement a policy in different ways. The two we will cover are:\n",
        "\n",
        "*   **Tabular Policy**: A simple lookup table. For each state, we store a list of action logits. This works perfectly for small environments like CliffWalking where we can list every possible state.\n",
        "*   **Neural Network Policy**: A more powerful function approximator that can handle environments with big or continuous state spaces (e.g., controlling a robot from camera images).\n",
        "\n",
        "We will start with a tabular policy, as it's the easiest to understand and implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkEu2OIvDSO"
      },
      "source": [
        "#### ✏️ **Exercise: Implement a tabular policy**\n",
        "\n",
        "Let's build a tabular policy called `GridPolicy`. The `forward` method has already been completed: it takes an observation (the agent's current location) and returns action logits.\n",
        "\n",
        "Your task is to instantiate the policy: create a learnable table of size `[number of states, number of actions]`. This table will hold the policy's logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3e-3y1mSEQF"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   First, create a tensor of random values using `torch.randn()`.\n",
        "\n",
        "* Think about the shape of this tabular policy: what is the shape of the table? Does it depend on the environment?\n",
        "\n",
        "*   To make this tensor a learnable parameter of the module, wrap the tensor you created inside `nn.Parameter()`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzkG0PavzoPw"
      },
      "outputs": [],
      "source": [
        "class GridPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    A policy for environments with a discrete state space (like a grid).\n",
        "    It's a simple table mapping each state to action logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states: int, num_actions: int):\n",
        "        super().__init__()\n",
        "        ################################################################\n",
        "        # Create a learnable parameter of size [num_states, num_actions].\n",
        "        # Your solution:\n",
        "        # self.policy = ...\n",
        "        ################################################################\n",
        "\n",
        "    def forward(self, obs: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given an observation (state), return action logits.\n",
        "        \"\"\"\n",
        "        action_logits = self.policy[obs]\n",
        "        return action_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yEY_1LWQEi4"
      },
      "outputs": [],
      "source": [
        "# Let's instantiate the Grid policy and print the action logits for a custom state.\n",
        "\n",
        "untrained_policy = GridPolicy(num_states=num_states, num_actions=num_actions)\n",
        "test_obs = 36  #@param\n",
        "logits = untrained_policy(test_obs)\n",
        "print(f\"Action logits for state {test_obs}: {logits.detach().numpy()}\")\n",
        "\n",
        "# We can also use PyTorch's Categorical distribution to see the probabilities\n",
        "# and sample an action.\n",
        "action_distribution = Categorical(logits=logits)\n",
        "probabilities = action_distribution.probs\n",
        "sampled_action = action_distribution.sample()\n",
        "\n",
        "print(f\"Probabilities: {probabilities.detach().numpy()}\")\n",
        "print(f\"Sampled Action: {sampled_action.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_8h5v3Wc6G1i"
      },
      "outputs": [],
      "source": [
        "#@title Watch your untrained GridPolicy *random* agent!\n",
        "\n",
        "print(\"Visualizing an untrained GridPolicy with random weights...\")\n",
        "visualise_policy(\n",
        "    policy=untrained_policy,\n",
        "    max_episode_steps=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dTXDq_CvNRR"
      },
      "source": [
        "# 🛠️ Practical 1: REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2WjNCCAuFPY"
      },
      "source": [
        "### The REINFORCE algorithm\n",
        "---\n",
        "\n",
        "We have a policy, but how does it actually learn which actions to take?\n",
        "\n",
        "We'll use a famous policy gradient algorithm called **REINFORCE**. The core idea is intuitive:\n",
        "\n",
        "> If a trajectory had a high return, increase the probability of the actions taken in that trajectory. If it had a low return, decrease their probability.\n",
        "\n",
        "This is a form of trial-and-error learning. We're \"reinforcing\" good behaviors.\n",
        "\n",
        "### Agent's goal: maximizing expected return\n",
        "---\n",
        "\n",
        "Mathematically, our goal is to find the policy parameters, $\\theta$, that maximize the expected total discounted return, which is our objective function $J(\\theta)$.\n",
        "\n",
        "$$\n",
        "\\max_{\\theta} J(\\theta) \\quad \\text{where} \\quad J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)]\n",
        "$$\n",
        "\n",
        "This means we want to find the parameters $\\theta$ that, on average, produce trajectories ($\\tau$) with the highest possible total return $G(\\tau)$. The standard way to maximize a function is **gradient ascent**: we calculate the gradient of the objective with respect to the parameters, $\\nabla_\\theta J(\\theta)$, and take a small step in that direction.\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta J(\\theta)\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PFKC1M56eJ"
      },
      "source": [
        "<details>\n",
        "\n",
        "<summary> Challenge: finding the Policy Gradient</summary>\n",
        "\n",
        "The tricky part is calculating the gradient $\\nabla_\\theta J(\\theta)$. Let's write out the expectation to see why:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] = \\nabla_\\theta \\sum_{\\tau} P(\\tau|\\theta) G(\\tau)\n",
        "$$\n",
        "\n",
        "Here, $P(\\tau|\\theta)$ is the probability of a specific trajectory $\\tau$ occurring under our policy $\\pi_\\theta$. We run into two major problems:\n",
        "\n",
        "1.  **The gradient is outside an expectation that depends on $\\theta$**. The distribution of trajectories we sample from, $P(\\tau|\\theta)$, changes as we change our policy parameters $\\theta$. We can't just move the gradient inside the expectation, because the thing we are averaging over depends on the variable we're differentiating with respect to.\n",
        "\n",
        "2.  **The trajectory probability depends on the unknown environment dynamics**. The probability of a trajectory is a product of the policy's action probabilities and the environment's transition probabilities:\n",
        "\n",
        "$$\n",
        "P(\\tau|\\theta) = p(s_0) \\prod_{t=0}^{T-1} \\underbrace{\\pi_\\theta(a_t|s_t)}_{\\text{Our policy}} \\cdot \\underbrace{p(s_{t+1}|s_t, a_t)}_{\\text{Environment}}\n",
        "$$\n",
        "\n",
        "To compute the gradient $\\nabla_\\theta P(\\tau|\\theta)$, we would need to know the environment dynamics, $p(s_{t+1}|s_t, a_t)$, which in most interesting problems is unknown and complex. We can't differentiate a function we don't know!\n",
        "\n",
        "We seem to be stuck. How can we get a gradient that depends only on our policy and is easy to estimate from experience? This is where the **Policy Gradient Theorem** comes to the rescue.\n",
        "\n",
        "The theorem provides a clever way to rewrite the gradient, turning an intractable problem into a solvable one. It accomplishes two things: it moves the gradient operator *inside* the expectation, and it eliminates the need for the environment model.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary> The Log Trick </summary>\n",
        "\n",
        "The key mathematical insight is a simple identity from calculus called the \"log-derivative trick\" or \"log-prob trick\": for any positive, differentiable function $f(x)$, its derivative can be written as:\n",
        "\n",
        "$$\n",
        "\\nabla f(x) = f(x) \\frac{\\nabla f(x)}{f(x)} = f(x) \\nabla \\log f(x)\n",
        "$$\n",
        "\n",
        "We can apply this to our trajectory probability, $P(\\tau|\\theta)$:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta P(\\tau|\\theta) = P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta)\n",
        "$$\n",
        "\n",
        "Now we can substitute this back into our gradient calculation:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\sum_{\\tau} P(\\tau|\\theta) G(\\tau) \\\\\n",
        "&= \\sum_{\\tau} \\nabla_\\theta P(\\tau|\\theta) G(\\tau) \\\\\n",
        "&= \\sum_{\\tau} P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) G(\\tau) \\\\\n",
        "&= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\cdot \\nabla_\\theta \\log P(\\tau|\\theta) \\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "This is a beautiful result! We've successfully moved the gradient operator inside the expectation. Now, we can estimate this gradient by simply running our policy and averaging the results—a **Monte Carlo** approach.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary> Getting rid of the environment model </summary>\n",
        "\n",
        "But what about the second problem? Doesn't $\\nabla_\\theta \\log P(\\tau|\\theta)$ still depend on the environment dynamics? Let's expand it.\n",
        "\n",
        "Remember, the log of a product is the sum of logs:\n",
        "\n",
        "$$\n",
        "\\log P(\\tau|\\theta) = \\log p(s_0) + \\sum_{t=0}^{T-1} \\left( \\log \\pi_\\theta(a_t|s_t) + \\log p(s_{t+1}|s_t, a_t) \\right)\n",
        "$$\n",
        "\n",
        "Now, let's take the gradient with respect to our policy parameters, $\\theta$:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\log P(\\tau|\\theta) = \\nabla_\\theta \\left[ \\log p(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T-1} \\log p(s_{t+1}|s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "The parameters $\\theta$ are only in our policy, $\\pi_\\theta$. The environment dynamics $p(s_{t+1}|s_t, a_t)$ and the initial state distribution $p(s_0)$ **do not depend on $\\theta$**. Therefore, their gradients are zero!\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\n",
        "$$\n",
        "\n",
        "This is the final piece of the puzzle. The gradient of the log-probability of a trajectory is just the sum of the gradients of the log-probabilities of the actions taken.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Further reading</summary>\n",
        "\n",
        "Check out [Lil'Log blogpost](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/) about policy gradient algorithms.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmzQ3iSJ5vl6"
      },
      "source": [
        "### From theory to practice: the REINFORCE update rule\n",
        "---\n",
        "\n",
        "Combining our results, we get the final expression for the policy gradient:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right]\n",
        "$$\n",
        "\n",
        "This is an expectation, which we can approximate by running our policy to collect a batch of $N$ episodes (trajectories) and then averaging the results.\n",
        "\n",
        "One final, crucial optimization is based on **causality**. The action taken at timestep $t$ can only affect rewards from timestep $t$ onwards. It cannot affect past rewards. Therefore, to credit an action $a_t$, we should only use the return starting from that step, $G_t$.\n",
        "\n",
        "Replacing the full trajectory return $G(\\tau)$ with $G_t$ for each action's gradient term reduces variance and leads to more stable and faster learning.\n",
        "\n",
        "This gives us our final, practical formula for the gradient, which is the core of the **REINFORCE** algorithm. We estimate it by averaging over all the steps in our batch of episodes:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T_i-1} G_{i,t} \\cdot \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t})\n",
        "$$\n",
        "\n",
        "👨‍💻 Let's break this down one last time:\n",
        "*   We collect a batch of $N$ trajectories using our current policy $\\pi_\\theta$.\n",
        "*   For each step $t$ in each trajectory $i$:\n",
        "    *   We calculate the return, $G_{i,t}$.\n",
        "    *   We calculate the gradient of the log-probability of the action we took, $\\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t})$. This term tells us which direction in parameter space would make that action more likely.\n",
        "    *   We scale this direction vector by the return $G_{i,t}$. If the future return was high, we push the parameters in that direction. If it was low (or negative), we push them in the opposite direction.\n",
        "*   We average all these scaled vectors together to get our final estimate of the policy gradient, and then update our parameters $\\theta$.\n",
        "\n",
        "✨ In the next exercise, you will implement the REINFORCE loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFgrdMgNnAXt"
      },
      "source": [
        "### Building a learning agent: implementation considerations\n",
        "---\n",
        "\n",
        "To keep our code clean and easy to understand, we'll separate our learning system into 3 components: the **Policy**, the **Agent**, and the **Algorithm**.\n",
        "\n",
        "1.  **Policy**: the \"brain\" of our operation.\n",
        "    *   **Role**: To decide which action to take in a given state. It maps a state to action logits. It contains the learnable parameters (the weights).\n",
        "    *   **Implementation**: The `GridPolicy` class. Later we'll implement  it as a neural network (`NNPolicy`).\n",
        "\n",
        "2.  **Agent**: the \"body\" or the complete entity that interacts with the world.\n",
        "    *   **Role**: It holds a policy and uses it to act in the environment. It's also responsible for collecting experience (the sequences of states, actions, and rewards). Crucially, it orchestrates the entire learning process.\n",
        "    *   **Implementation**: We will soon create a class called `ReinforceAgent`.\n",
        "\n",
        "3.  **Learning Algorithm**: the \"training rule\" that enables learning.\n",
        "    *   **Role**: It takes the experience collected by the Agent and uses it to update the Policy's parameters, making the policy better over time.\n",
        "    *   **Implementation**: The algorithm is the logic *inside* our Agent's `.update()` method. In this tutorial, the algorithm we will implement is called **REINFORCE**.\n",
        "\n",
        "This modular design is very convenient. Our `ReinforceAgent` will be able to train *any* kind of policy (`Grid_Policy`, `NN_Policy`, etc.) because the learning algorithm is separate from the policy's internal structure.\n",
        "\n",
        "```\n",
        "┌────────────────────────────────┐\n",
        "│   ReinforceAgent               │\n",
        "│  (The complete learner)        │\n",
        "│                                │\n",
        "│   ┌────────────────────────┐   │  <── The Agent HOLDS a Policy\n",
        "│   │         Policy         │   │\n",
        "│   │ (e.g., Grid_Policy or  │   │\n",
        "│   │      NN_Policy)        │   │\n",
        "│   └────────────────────────┘   │\n",
        "│                                │\n",
        "│   → select_action() uses Policy│  <── The Agent USES the Policy to act\n",
        "│   → update() implements        │  <── The Agent USES the Algorithm to learn\n",
        "│     REINFORCE Algorithm        │\n",
        "└────────────────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbBexHMgQT7l"
      },
      "source": [
        "#### ✏️ **Exercise: Implement the REINFORCE Loss**\n",
        "\n",
        "Now we'll create the `ReinforceAgent`. The Agent takes as input a policy to choose actions (we will re-use the `GridPolicy` you implemented above) and implements the logic for updating the policy's parameters using the REINFORCE algorithm.\n",
        "\n",
        "Your task is to implement the REINFORCE loss using the `get_returns` function you already implemented above.\n",
        "\n",
        "> Note: Optimizers in PyTorch perform gradient *descent* (minimization). As we saw, the objective is to *maximize* $\\log \\pi_\\theta(a_t|s_t) * G_t$, so we'll implement our loss as the *negative* of this objective:\n",
        "\n",
        "$$ \\text{REINFORCE loss} = - \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t | s_t) \\cdot G_t $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99AZMs1tTKPf"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   Your goal is to translate the REINFORCE loss formula into code.\n",
        "\n",
        "*   Start by multiplying the `log_probs` tensor with the `returns` tensor. This operation is element-wise.\n",
        "\n",
        "*   Sum up all the elements of the resulting tensor to get a single scalar value. The `torch.sum()` function is what you need.\n",
        "\n",
        "*   Remember to put a negative sign in front of the final sum. This is because optimizers minimize a loss, but we want to maximize our objective.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOkY5hQn0EKo"
      },
      "outputs": [],
      "source": [
        "class ReinforceAgent(nn.Module):\n",
        "    \"\"\"Implements the REINFORCE algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, policy: nn.Module, gamma: float = 0.99):\n",
        "        super().__init__()\n",
        "        self.policy = policy\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, obs: int) -> tuple[int, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Selects an action and returns it along with its log-probability.\n",
        "\n",
        "        Args:\n",
        "            obs: The current observation (state) from the environment.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - int: The action selected by the policy.\n",
        "            - torch.Tensor: The log-probability of the selected action.\n",
        "        \"\"\"\n",
        "        # Get action logits from the policy network.\n",
        "        logits = self.policy(obs)\n",
        "\n",
        "        # Create a probability distribution directly from logits.\n",
        "        distribution = Categorical(logits=logits)\n",
        "\n",
        "        # Sample an action from the distribution and get log-prob.\n",
        "        action = distribution.sample()\n",
        "        log_prob_action = distribution.log_prob(action)\n",
        "\n",
        "        return action.item(), log_prob_action\n",
        "\n",
        "    def calculate_reinforce_loss(\n",
        "        self, returns: torch.Tensor, log_probs: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculates the REINFORCE loss for a single episode.\n",
        "\n",
        "        Args:\n",
        "            returns: A tensor of discounted returns for one episode.\n",
        "            log_probs: A tensor of the log_probs for the actions in that episode.\n",
        "\n",
        "        Returns:\n",
        "            A tensor representing the calculated loss for the episode.\n",
        "        \"\"\"\n",
        "        ################################################################\n",
        "        # Calculate the REINFORCE loss.\n",
        "        # Your solution:\n",
        "        # loss = ...\n",
        "        ################################################################\n",
        "        return loss\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        batch_log_probs: list[list[torch.Tensor]],\n",
        "        batch_rewards: list[list[float]],\n",
        "        optimizer: optim.Optimizer,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Performs the learning step for a batch of episodes.\n",
        "\n",
        "        For each episode in the batch:\n",
        "            * Calculates discounted returns from the stored rewards.\n",
        "            * Stacks the log_probs into a tensor.\n",
        "            * Computes the REINFORCE loss for the episode.\n",
        "            * Appends the episode loss to the list of losses.\n",
        "        Then, it averages the losses across the batch and performs a gradient\n",
        "        ascent step to update the policy.\n",
        "        \"\"\"\n",
        "        batch_loss = []\n",
        "        # Iterate over each episode in the batch.\n",
        "        for episode_rewards, episode_log_probs in zip(batch_rewards, batch_log_probs):\n",
        "            returns = get_returns(gamma=self.gamma, rewards=episode_rewards)\n",
        "            log_probs = torch.stack(episode_log_probs)\n",
        "            episode_loss = self.calculate_reinforce_loss(returns, log_probs)\n",
        "            batch_loss.append(episode_loss)\n",
        "        average_batch_loss = torch.stack(batch_loss).mean()\n",
        "\n",
        "        # Perform the gradient ascent step\n",
        "        optimizer.zero_grad()\n",
        "        average_batch_loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sPH5SxXVvMfj"
      },
      "outputs": [],
      "source": [
        "#@title Test your implementation of the `calculate_reinforce_loss` method.\n",
        "\n",
        "def test_agent_loss_method():\n",
        "    agent = ReinforceAgent(policy=untrained_policy)\n",
        "    returns = torch.tensor([2.0, -0.5, -1.0])\n",
        "    log_probs = torch.tensor([-0.5, -1.0, -0.2])\n",
        "    calculated_loss = agent.calculate_reinforce_loss(returns, log_probs)\n",
        "\n",
        "    # Expected: -sum([-0.5*2.0, -1.0*-0.5, -0.2*-1.0]) = -sum([-1.0, 0.5, 0.2]).\n",
        "    expected_loss = torch.tensor(0.3)\n",
        "\n",
        "    if torch.isclose(calculated_loss, expected_loss):\n",
        "        print(\"✅ TEST PASSED!\")\n",
        "        print(f\"Your loss: {calculated_loss.item():.1f}, \"\n",
        "              f\"Expected loss: {expected_loss.item():.1f}\")\n",
        "    else:\n",
        "        print(\"❌ TEST FAILED!\")\n",
        "        print(f\"Your loss: {calculated_loss.item():.1f}, \"\n",
        "              f\"Expected loss: {expected_loss.item():.1f}\")\n",
        "\n",
        "# Run the test\n",
        "test_agent_loss_method()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMNVZp2OQ37U"
      },
      "source": [
        "### Training loop for the REINFORCE Agent\n",
        "\n",
        "We now have all the components. The final step is to orchestrate the agent-environment interaction. The loop will look like this:\n",
        "\n",
        "1.  Play a number of episodes (`episodes_per_batch`).\n",
        "2.  In each episode:\n",
        "    -   Reset the environment.\n",
        "    -   Loop until the episode is `done` (terminated or truncated):\n",
        "        -   The agent chooses an action.\n",
        "        -   The environment gives a new state and reward.\n",
        "        -   Store the reward in the agent's history.\n",
        "3.  After collecting data from the batch, call `agent.update()` to learn.\n",
        "4.  Repeat for a number of updates.\n",
        "\n",
        "Read through the training loop implementation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blQxw3jN6YyY"
      },
      "outputs": [],
      "source": [
        "def train_agent(\n",
        "    env: gym.Env,\n",
        "    agent: ReinforceAgent,\n",
        "    optimizer: optim.Optimizer,\n",
        "    num_updates: int,\n",
        "    episodes_per_batch: int,\n",
        "    max_episode_steps: int,\n",
        "    convergence_threshold: float,\n",
        "    log_frequency: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    A generic function to train a REINFORCE agent.\n",
        "\n",
        "    Args:\n",
        "        env: The gymnasium environment to train in.\n",
        "        agent: The ReinforceAgent to train.\n",
        "        optimizer: The optimizer for the agent's policy.\n",
        "        num_updates: The total number of policy updates to perform.\n",
        "        episodes_per_batch: The number of episodes to run per policy update.\n",
        "        max_episode_steps: The maximum steps allowed per episode.\n",
        "        convergence_threshold: The average reward to aim for to \"solve\" the env.\n",
        "        log_frequency: How often (in updates) to print progress.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of avg_rewards for plotting.\n",
        "    \"\"\"\n",
        "    avg_rewards = []\n",
        "\n",
        "    print(f\"Starting training for {agent.policy.__class__.__name__}...\")\n",
        "    pbar = tqdm(range(num_updates), mininterval=2)\n",
        "    for i in pbar:\n",
        "        # These lists will store the experience for the entire batch.\n",
        "        batch_rewards = []\n",
        "        batch_log_probs = []\n",
        "        batch_episode_rewards = []\n",
        "\n",
        "        for _ in range(episodes_per_batch+1):\n",
        "            episode_rewards = []\n",
        "            episode_log_probs = []\n",
        "            total_episode_reward = 0\n",
        "            state, _ = env.reset()\n",
        "\n",
        "            for step in range(max_episode_steps):\n",
        "                # This is to handle state differences between environments:\n",
        "                # e.g., int for CliffWalking, numpy array for CartPole.\n",
        "                if isinstance(state, np.ndarray):\n",
        "                    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "                else:\n",
        "                    state_tensor = state\n",
        "\n",
        "                # Get action & log_prob from the agent.\n",
        "                action, log_prob = agent(state_tensor)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # *** Apply reward shaping for faster convergence ***\n",
        "                shaped_reward = get_shaped_reward(env, reward, state, next_state)\n",
        "\n",
        "                # Store experience for the current episode.\n",
        "                episode_rewards.append(shaped_reward)\n",
        "                episode_log_probs.append(log_prob)\n",
        "                total_episode_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # After the episode is done, add its history to the batch history.\n",
        "            batch_rewards.append(episode_rewards)\n",
        "            batch_log_probs.append(episode_log_probs)\n",
        "            batch_episode_rewards.append(total_episode_reward)\n",
        "\n",
        "        # 2. Update the agent's policy using the collected batch of experience.\n",
        "        agent.update(batch_log_probs, batch_rewards, optimizer)\n",
        "\n",
        "        # 3. Logging.\n",
        "        avg_reward_for_batch = np.mean(batch_episode_rewards)\n",
        "        avg_rewards.append(avg_reward_for_batch)\n",
        "\n",
        "        if i % log_frequency == 0:\n",
        "            print()\n",
        "        pbar.set_postfix(avg_reward=f\"{avg_reward_for_batch:.2f}\")\n",
        "\n",
        "        # 4. Check for convergence.\n",
        "        if avg_reward_for_batch > convergence_threshold:\n",
        "            print(f\"\\nSolved at update {i}!\")\n",
        "            break\n",
        "\n",
        "    return avg_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AJmQBetLfqc"
      },
      "source": [
        "Let's run the training loop for the ReinforceAgent training a GridPolicy.\n",
        "\n",
        "There's nothing for you to code, but feel free to experiment with the hyperparameters! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pttE4Ep4LbVh"
      },
      "outputs": [],
      "source": [
        "# --- Hyperparameters ---\n",
        "TABULAR_LEARNING_RATE = 0.1  # @param\n",
        "TABULAR_GAMMA = 0.99  # @param\n",
        "TABULAR_NUM_UPDATES = 700  # @param\n",
        "TABULAR_EPISODES_PER_BATCH = 10  # @param\n",
        "TABULAR_MAX_EPISODE_STEPS = 100  # @param\n",
        "\n",
        "\n",
        "# --- Initialization ---\n",
        "grid_policy = GridPolicy(num_states, num_actions)\n",
        "grid_agent = ReinforceAgent(grid_policy, gamma=TABULAR_GAMMA)\n",
        "grid_optimizer = optim.Adam(grid_policy.parameters(), lr=TABULAR_LEARNING_RATE)\n",
        "\n",
        "\n",
        "# --- Run Training ---\n",
        "grid_avg_rewards = train_agent(\n",
        "    env=env,\n",
        "    agent=grid_agent,\n",
        "    optimizer=grid_optimizer,\n",
        "    num_updates=TABULAR_NUM_UPDATES,\n",
        "    episodes_per_batch=TABULAR_EPISODES_PER_BATCH,\n",
        "    max_episode_steps=TABULAR_MAX_EPISODE_STEPS,\n",
        "    convergence_threshold=-15,\n",
        "    log_frequency=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWG8wGGfREG8"
      },
      "source": [
        "Let's plot the learning progress. You should see the average reward increase 📈."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tBNJGQ7fQ3eZ"
      },
      "outputs": [],
      "source": [
        "#@title Plotting learning progress for the Reinforce Agent with GridPolicy.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(grid_avg_rewards)\n",
        "plt.title(\"Average Episodic Reward (GridPolicy)\")\n",
        "plt.xlabel(\"Update Batch\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bbdbDzb7ROyf"
      },
      "outputs": [],
      "source": [
        "#@title Watch your trained GridPolicy REINFORCE agent!\n",
        "\n",
        "visualise_policy(\n",
        "    policy=grid_agent.policy,\n",
        "    max_episode_steps=TABULAR_MAX_EPISODE_STEPS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrPCPACBsVOH"
      },
      "source": [
        "### A More General Approach: The Neural Network Policy\n",
        "\n",
        "Our `Grid_Policy` (a lookup table) worked, but for larger or continuous state spaces, tables quickly become impractical.\n",
        "\n",
        "We can replace our specialized table with a general-purpose **function approximator**: a neural network (NN) 🕸️. While it may seem like overkill here, it's a good exercise to show how we can apply these more powerful models.\n",
        "\n",
        "Neural networks require inputs to be vectors, not single integers. So we need to convert a state like `27` into a vector format.\n",
        "\n",
        "The standard method is **one-hot encoding**: we create a vector with one entry for each possible state (e.g., 48 total), all set to `0`, and set the element at index `27` to `1`.\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "state = 27  \n",
        "=>  \n",
        "one_hot_vector = [0, 0, ..., 1, 0, ...]  # 1 at index 27\n",
        "```\n",
        "\n",
        "Now, let’s build a policy that performs this encoding and feeds the result into a simple MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIWjbRU_wB4k"
      },
      "source": [
        "#### ✏️ **Exercise: Define the NN Architecture**\n",
        "\n",
        "First, complete the `__init__` method by defining a simple 2-3 layer MLP using `nn.Sequential`. Make sure that your input dimension will be the total number of states. Then, complete the `forward` method. This is where you'll convert the integer observation into a one-hot vector before passing it to your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frG9Yw-WTblM"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "For the `__init__` method:\n",
        "\n",
        "*   Use `nn.Sequential` to define the network architecture.\n",
        "*   Your **first** layer should be an `nn.Linear` that maps `num_states` to a hidden size (e.g., 32).\n",
        "*   Follow this with an `nn.ReLU()` activation function.\n",
        "*   Your **last** layer should be an `nn.Linear` that maps the hidden size to `num_actions`.\n",
        "*   Feel free to add extra layers and play with the number of hidden sizes.\n",
        "\n",
        "For the `forward` method:\n",
        "\n",
        "*   Use the `F.one_hot()` function to convert the integer observation into a one-hot vector.\n",
        "*   Make sure to set the `num_classes` argument in `F.one_hot()` to `self.num_states`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qHMx9xYpcoi"
      },
      "outputs": [],
      "source": [
        "class NNPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    A policy parameterized by a neural network. It uses one-hot encoding\n",
        "    to handle discrete state spaces.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_states: int, num_actions: int):\n",
        "        super().__init__()\n",
        "        self.num_states = num_states\n",
        "\n",
        "        ################################################################\n",
        "        # Build a simple 2-3 layer MLP for the policy.\n",
        "        # Your solution:\n",
        "        # self.policy_net = ...\n",
        "        ################################################################\n",
        "\n",
        "    def forward(self, obs: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass. It takes an integer observation,\n",
        "        converts it to a one-hot vector, and passes it through the network.\n",
        "        \"\"\"\n",
        "        # First, convert the integer observation to a tensor\n",
        "        obs_tensor = torch.tensor(obs, dtype=torch.long)\n",
        "\n",
        "        ################################################################\n",
        "        # Convert the observation tensor to a one-hot vector.\n",
        "        # Your solution:\n",
        "        # one_hot_obs = ...\n",
        "        ################################################################\n",
        "\n",
        "        # Cast the result to a float tensor.\n",
        "        one_hot_obs = one_hot_obs.float()\n",
        "\n",
        "        # Pass the one-hot vector through the network to get logits.\n",
        "        action_logits = self.policy_net(one_hot_obs)\n",
        "        return action_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSijgWWp1V2C"
      },
      "source": [
        "### Training loop for the `NNPolicy` REINFORCE Agent\n",
        "\n",
        "Just like before, our modular `ReinforceAgent` can use this new policy without any changes to its own code. We simply instantiate the `NNPolicy` and run the same training loop.\n",
        "\n",
        "In the code block below, we initialize the `NNPolicy`, the `ReinforceAgent`, and the optimizer. Then, run the training loop to solve `CliffWalking` with your new neural network policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSQ4tD3U12G1"
      },
      "outputs": [],
      "source": [
        "# --- Hyperparameters for NN on CliffWalking ---\n",
        "NN_LEARNING_RATE = 0.001  # @param\n",
        "NN_GAMMA = 0.99  # @param\n",
        "NN_NUM_UPDATES = 700  # @param\n",
        "NN_EPISODES_PER_BATCH = 10  # @param\n",
        "NN_MAX_EPISODE_STEPS = 100  # @param\n",
        "\n",
        "\n",
        "# --- Initialization ---\n",
        "nn_policy = NNPolicy(num_states, num_actions)\n",
        "nn_agent = ReinforceAgent(nn_policy, gamma=NN_GAMMA)\n",
        "nn_optimizer = optim.Adam(nn_policy.parameters(), lr=NN_LEARNING_RATE)\n",
        "\n",
        "\n",
        "# --- Run Training ---\n",
        "nn_avg_rewards = train_agent(\n",
        "    env=env,\n",
        "    agent=nn_agent,\n",
        "    optimizer=nn_optimizer,\n",
        "    num_updates=NN_NUM_UPDATES,\n",
        "    episodes_per_batch=NN_EPISODES_PER_BATCH,\n",
        "    max_episode_steps=NN_MAX_EPISODE_STEPS,\n",
        "    convergence_threshold=-14,\n",
        "    log_frequency=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uNb7ZkuOtrYl"
      },
      "outputs": [],
      "source": [
        "#@title Plotting learning progress for the Reinforce Agent with NNPolicy.\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(nn_avg_rewards)\n",
        "plt.title(\"Average Episodic Reward (NN Policy)\")\n",
        "plt.xlabel(\"Update Batch\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "06ekFpwPt6lj"
      },
      "outputs": [],
      "source": [
        "#@title Watch your NN-based agent!\n",
        "\n",
        "visualise_policy(\n",
        "    policy=nn_agent.policy,\n",
        "    max_episode_steps=500,\n",
        "    folder=\"./videos_nn\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhVYbFxcuVtP"
      },
      "source": [
        "### **Conclusion**\n",
        "\n",
        "Congratulations! 🥳\n",
        "\n",
        "You now have a solid foundation in policy gradient methods. From here, you can explore more advanced algorithms like **Actor-Critic (A2C)** and **Proximal Policy Optimization (PPO)**, which build upon the core ideas you've learned today to offer even better stability and performance. Happy learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V454B476owZN"
      },
      "source": [
        "# 🛠️ Practical 2 (Advanced): A2C and PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vILvAAmWZi0"
      },
      "source": [
        "In this practical, we will learn about [Advantage Actor-Critic(A2C)](https://arxiv.org/abs/1602.01783) and [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347).\n",
        "\n",
        "For this part, we will use the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment.\n",
        "\n",
        "**[CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)** is a classic control task where the goal is to balance a pole upright on a moving cart. The agent observes a 4-dimensional continuous state (cart position, cart velocity, pole angle, and pole angular velocity) and chooses between 2 discrete actions: move the cart left or right. The episode ends when the pole falls past a certain angle, the cart moves out of bounds, or 500 time steps elapse. The agent receives a reward of +1 for each time step the pole remains balanced, with a maximum possible score of 500 per episode.\n",
        "\n",
        "\n",
        "![cart-pole](https://gymnasium.farama.org/_images/cart_pole.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mnjZHohosBtc"
      },
      "outputs": [],
      "source": [
        "#@title Load the \"CartPole\" environment.\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "num_actions = env.action_space.n\n",
        "input_dim = env.observation_space.shape[0]\n",
        "\n",
        "print(f\"Observation space (continuous): {env.observation_space.shape}\")\n",
        "print(f\"Action space: {env.action_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Sk9vGhNQEV"
      },
      "source": [
        "### Problems with REINFORCE\n",
        "---\n",
        "\n",
        "The REINFORCE algorithm has one major drawback: **high variance** in its gradient estimates.\n",
        "\n",
        "Let's look at the update rule again. The policy gradient is scaled by the return $G_{i,t}$:\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T_i-1} \\color{red}{G_{i,t}} \\color{black}{\\cdot \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t})}\n",
        "$$\n",
        "\n",
        "This single scalar value, $G_{i,t}$, is used to credit every action from step $t$ onwards. This creates two problems related to what is known as **credit assignment**:\n",
        "\n",
        "1.  **Good actions in a bad trajectory get penalized.** Imagine an agent takes one great action but then makes a series of blunders, leading to a low total return. The great action will still be discouraged because the overall outcome was poor.\n",
        "2.  **Bad actions in a good trajectory get rewarded.** Conversely, a single lucky action at the end of an episode can result in a high return, causing all preceding actions (even poor ones) to be reinforced.\n",
        "\n",
        "Because the Monte Carlo return $G_{i,t}$ is the sum of many potentially random events, it can fluctuate wildly from one episode to the next. This makes the learning signal very noisy. To overcome this noise, REINFORCE often requires a large number of samples (episodes), making it **sample inefficient**.\n",
        "\n",
        "So, how can we create a more stable and reliable learning signal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lZQd-27AG4o"
      },
      "source": [
        "### Actor-Critic methods\n",
        "---\n",
        "\n",
        "The key idea is to change our perspective. Instead of judging an action by its raw return $G_t$, we ask a better question:\n",
        "\n",
        "> How much better or worse was this return than what we *expected* to get from this state?\n",
        "\n",
        "To answer this, we need to formalize the idea of \"what we expected to get\". This is precisely the job of the **state-value function**, denoted as $V(s)$. The value function $V^\\pi(s)$ answers the question: \"Starting from state $s$, what is the average total return I can expect to get if I follow my current policy $\\pi$ from this point forward?\"\n",
        "\n",
        "Mathematically, it's defined as the **expected return, given the starting state**:\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_{\\pi}[G_t | S_t = s]\n",
        "$$\n",
        "In short, $V(s)$ is a measure of how \"good\" a state is. A state with a high value is promising because we expect to accumulate a lot of future reward from there.\n",
        "\n",
        "With this concept, we have the perfect **baseline** for our calculation. We can compare the *actual* return $G_t$ we got with the *expected* return $V(s_t)$. This difference is called the **Advantage**.\n",
        "\n",
        "$$\n",
        "A(s_t, a_t) \\approx G_t - V(s_t)\n",
        "$$\n",
        "\n",
        "The Advantage $A(s_t, a_t)$ tells us how much better taking action $a_t$ was compared to the average action from state $s_t$.\n",
        "*   If $A(s_t, a_t) > 0$, the action led to a better-than-expected outcome.\n",
        "*   If $A(s_t, a_t) < 0$, the action led to a worse-than-expected outcome.\n",
        "\n",
        "\n",
        "This signal is much less noisy. A return of `+10` might be fantastic in a state where you expect `0` (Advantage = +10), but poor in a state where you expect `+20` (Advantage = -10). The Advantage centers the learning signal around zero, dramatically reducing variance and stabilizing training.\n",
        "\n",
        "This leads us to **Actor-Critic** methods:\n",
        "-   The **Actor** is the policy ($\\pi_\\theta$), which decides which action to take. It learns using the Advantage.\n",
        "-   The **Critic** is the value function ($V_\\phi$), which estimates the value of each state. Its job is to provide the baseline for the Actor's Advantage calculation.\n",
        "\n",
        "Both the Actor and the Critic are neural networks with parameters $\\theta$ and $\\phi$, respectively, that we train simultaneously.\n",
        "\n",
        "To train them, we combine their objectives into a single loss function:\n",
        "\n",
        "1.  **Actor Loss ($L_{actor}$)**: This is the policy gradient loss, but using the Advantage instead of $G_t$. We want to maximize the log-probability of actions, scaled by their advantage. (Note: the negative sign is for gradient descent).\n",
        "\n",
        "    $$L_{actor} = - \\mathbb{E}[A(s_t, a_t) \\cdot \\log \\pi_\\theta(a_t|s_t)]$$\n",
        "\n",
        "2.  **Critic Loss ($L_{critic}$)**: The Critic's job is to make its value estimate $V_\\phi(s_t)$ as close as possible to the observed return $G_t$. This is a standard regression problem, so we use the Mean Squared Error.\n",
        "\n",
        "    $$L_{critic} = \\mathbb{E}[(G_t - V_\\phi(s_t))^2]$$\n",
        "\n",
        "The total loss we minimize is a weighted sum of the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSZ99dRbAuKi"
      },
      "source": [
        "### The Actor-Critic Policy\n",
        "---\n",
        "\n",
        "Our policy network now needs to produce 2 outputs from a single state:\n",
        "1.  A probability distribution over actions (for the Actor).\n",
        "2.  A single value estimating the quality of the state (for the Critic).\n",
        "\n",
        "A common and effective architecture is to use a shared \"body\" network that processes the observation, followed by two separate \"heads\" for the policy and value outputs. This allows the network to learn a common representation of the state that is useful for both tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COeQLfaANZ0N"
      },
      "source": [
        "#### ✏️ **Exercise: Implement an Actor-Critic NN Policy**\n",
        "\n",
        "Complete the architecture of the `NN_Policy_Value` class below. Since we are using `CartPole-v1`, the observation is a 4-dimensional vector representing (cart position, cart velocity, pole angle, and pole angular velocity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y7-4KKMPnaT"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   The shared body can be an `nn.Sequential` that contains an `nn.Linear(input_dim, hidden_size)` layer followed by an `nn.ReLU()`.\n",
        "\n",
        "*   The policy head (the actor) is a separate `nn.Linear` layer that maps the `hidden_size` features to `num_actions`.\n",
        "\n",
        "*   The value head (the critic) is another `nn.Linear` layer that maps the `hidden_size` features to a single output of size 1.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gClFVeypAxfO"
      },
      "outputs": [],
      "source": [
        "class NN_Policy_Value(nn.Module):\n",
        "    \"\"\"\n",
        "    An actor-critic policy network with a shared body and separate heads.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,  # this will be `env.observation_space.shape`\n",
        "        num_actions: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        ################################################################\n",
        "        # Your solution:\n",
        "        # 1. Define the shared network body.\n",
        "        # self.hidden_body = ...\n",
        "\n",
        "        # 2. Define the policy head for the Actor.\n",
        "        # self.policy_head = ...\n",
        "\n",
        "        # 3. Define the value head for the Critic.\n",
        "        # self.value_head = ...\n",
        "        ################################################################\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Performs the forward pass.\n",
        "\n",
        "        Args:\n",
        "            obs: A tensor of state indices.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "            - torch.Tensor: Action logits from the policy head.\n",
        "            - torch.Tensor: The state value from the value head.\n",
        "        \"\"\"\n",
        "        # First, one-hot encode the discrete state observation.\n",
        "        if obs.dim() == 0:  # Handle single integer observation.\n",
        "            obs = obs.unsqueeze(0)\n",
        "\n",
        "        # Pass through the shared body.\n",
        "        hidden_features = self.hidden_body(obs)\n",
        "\n",
        "        # Get action logits and state value from the heads.\n",
        "        action_logits = self.policy_head(hidden_features)  # [B, num_actions].\n",
        "        state_value = self.value_head(hidden_features)  # [B, 1].\n",
        "\n",
        "        return action_logits, state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMcyG7DAw9N"
      },
      "source": [
        "### The A2C (Advantage Actor-Critic) Agent\n",
        "---\n",
        "\n",
        "Let's build the `A2CAgent` agent.\n",
        "\n",
        "For the policy gradient loss, use the Advantage instead of $G_t$. For learning the state-value function $V_\\phi(s_t)$, we apply a Mean Squared Error to bring it closer to the observed return $G_t$. The total loss we minimize is their (weighted) sum.\n",
        "\n",
        "$$Loss_{A2C} = -\\mathbb{E}[A(s_t, a_t) \\cdot \\log \\pi_\\theta(a_t|s_t)] + \\alpha * \\mathbb{E}[(G_t - V_\\phi(s_t))^2]$$\n",
        "\n",
        "where $\\alpha$ is the weight of the value loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txH8SIE8A65d"
      },
      "source": [
        "#### ✏️ **Exercise: Implement the A2C Loss**\n",
        "\n",
        "Complete the `update` method in the `A2CAgent` below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwwZ90gsT30-"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   The advantage is the difference between the actual `returns` and the critic's predicted `values`.\n",
        "\n",
        "*   The policy loss is the negative mean of `log_probs` multiplied by the `advantage`. Use `advantage.detach()` to stop gradients from flowing back into the value network from this loss.\n",
        "\n",
        "*  The value loss measures the critic's prediction error. Use `F.mse_loss()` to calculate the mean squared error between the `values` and `returns`.\n",
        "\n",
        "*   The final loss is the `policy_loss` plus the `value_loss` scaled by `self._value_loss_weight`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgOY4r2xA7Fq"
      },
      "outputs": [],
      "source": [
        "class A2CAgent(ReinforceAgent):\n",
        "    \"\"\"Implements the Advantage Actor-Critic (A2C) algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, policy: NN_Policy_Value, gamma: float = 0.99, value_loss_weight: float = 0.5):\n",
        "        super().__init__(policy=policy, gamma=gamma)\n",
        "        self._value_loss_weight = value_loss_weight\n",
        "\n",
        "    def forward(self, obs: torch.Tensor) -> tuple[int, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Selects an action, its log-probability, and the state value.\"\"\"\n",
        "        action_logits, value = self.policy(obs)\n",
        "        distribution = Categorical(logits=action_logits)\n",
        "        action = distribution.sample()\n",
        "        log_prob = distribution.log_prob(action)\n",
        "        return action.item(), log_prob, value.squeeze(-1)\n",
        "\n",
        "    def update(self, batch_data: dict, optimizer: torch.optim.Optimizer):\n",
        "        \"\"\"Performs the learning step for a batch of episodes.\"\"\"\n",
        "        # Unpack data for the whole batch, collected by the training loop\n",
        "        log_probs = torch.cat(batch_data['log_probs'])\n",
        "        values = torch.cat(batch_data['values'])\n",
        "        returns = torch.cat(batch_data['returns'])\n",
        "\n",
        "        ################################################################\n",
        "        # Your solution:\n",
        "        # 1. Calculate the advantage.\n",
        "        # advantage = ...\n",
        "\n",
        "        # 2. Calculate the policy loss (actor loss).\n",
        "        # We use .mean() for stability across batch sizes.\n",
        "        # policy_loss = ...\n",
        "\n",
        "        # 3. Calculate the value loss (critic loss).\n",
        "        # value_loss = ...\n",
        "\n",
        "        # 4. Combine the losses.\n",
        "        # loss = ...\n",
        "        ################################################################\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjm__LqRBC0h"
      },
      "source": [
        "### Improving A2C with Entropy Regularization\n",
        "\n",
        "A common problem in RL is an agent that becomes too confident in a sub-optimal policy too early, failing to explore other, potentially better options. To encourage exploration, we can add an **entropy bonus** to our loss function.\n",
        "\n",
        "**Entropy** is a measure of randomness or uncertainty in a probability distribution. A high-entropy policy is more random (exploratory), while a low-entropy policy is more deterministic (exploitative).\n",
        "\n",
        "By adding the policy's entropy to the objective (or subtracting it from the loss), we encourage the agent to maintain a degree of randomness, preventing it from collapsing into a single, greedy action too soon.\n",
        "\n",
        "$$ \\text{Loss}_{final} = \\text{Loss}_{A2C} - \\beta \\cdot H(\\pi_\\theta(s_t)) $$\n",
        "\n",
        "where `H` is the entropy and `β` is a small coefficient to weigh the entropy bonus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxBbr48nBC5h"
      },
      "source": [
        "#### ✏️ **Exercise: Implement the Entropy Bonus**\n",
        "\n",
        "We can implement this by creating a new agent that inherits from `A2CAgent` and overrides the `update` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81MJlTTOUWxK"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   To calculate entropy for the batch, you first need to get the action distributions for all `states` that were visited. Pass the `states` tensor through `self.policy` to get the logits.\n",
        "\n",
        "*   Create a `Categorical` distribution from the logits. This distribution object has a built-in `.entropy()` method.\n",
        "\n",
        "*   Call `.entropy().mean()` on your distribution to get the average entropy for the batch.\n",
        "\n",
        "*   The final loss is `policy_loss + value_loss - (entropy_weight * entropy)`. We subtract the entropy term because maximizing entropy is equivalent to minimizing negative entropy.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDF-xXNMBHfH"
      },
      "outputs": [],
      "source": [
        "class A2CEntropyAgent(A2CAgent):\n",
        "    \"\"\"Implements A2C with an entropy bonus for exploration.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: NN_Policy_Value,\n",
        "        value_loss_weight: float = 0.5,\n",
        "        entropy_weight: float = 0.01,\n",
        "    ):\n",
        "        super().__init__(policy, value_loss_weight=value_loss_weight)\n",
        "        self._entropy_weight = entropy_weight\n",
        "\n",
        "    def update(self, batch_data: dict, optimizer: torch.optim.Optimizer):\n",
        "        \"\"\"Performs the learning step, adding an entropy bonus.\"\"\"\n",
        "        log_probs = torch.cat(batch_data['log_probs'])\n",
        "        values = torch.cat(batch_data['values'])\n",
        "        returns = torch.cat(batch_data['returns'])\n",
        "        states = torch.cat(batch_data['states'])\n",
        "\n",
        "        advantage = returns - values\n",
        "        policy_loss = -(log_probs * advantage.detach()).mean()\n",
        "        value_loss = F.mse_loss(values, returns)\n",
        "\n",
        "        ################################################################\n",
        "        # Your solution:\n",
        "        # 1. Get the action logits for all visited states to compute entropy.\n",
        "        # logits, _ = ...\n",
        "\n",
        "        # 2. Calculate the entropy bonus.\n",
        "        # entropy_bonus = ...\n",
        "\n",
        "        # 3. Combine the losses, including the entropy bonus.\n",
        "        # loss = ...\n",
        "        ################################################################\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4YPohpMBDOZ"
      },
      "source": [
        "### Stabilizing with PPO (Proximal Policy Optimization)\n",
        "\n",
        "Even with advantage estimation and entropy bonus, policy gradient methods can be unstable. A single bad update step can drastically change the policy, leading to a performance collapse that is difficult to recover from.\n",
        "\n",
        "**PPO** introduces a simple idea to prevent this: it constrains how much the new policy can differ from the old one at each update. It achieves this by \"clipping\" the objective function.\n",
        "\n",
        "The core idea is to use a ratio:\n",
        "\n",
        "$$w_t(\\theta) = \\pi_\\theta(a_t|s_t) / \\pi_{\\theta_{old}}(a_t|s_t)$$\n",
        "\n",
        "that measures how much the probability of an action has changed.\n",
        "\n",
        "The PPO loss then takes the minimum of the normal objective and a \"clipped\" version:\n",
        "\n",
        "$$ L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min(w_t(\\theta) \\hat{A}_t, \\text{clip}(w_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t) \\right] $$\n",
        "\n",
        "This clipping prevents the policy from changing too aggressively, especially when the advantage is large, leading to more stable training.\n",
        "\n",
        "> PPO is still a state-of-the-art algorithm used widely today.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zionrh_BQYH"
      },
      "source": [
        "#### ✏️ **Exercise 4: Implement the PPO Loss**\n",
        "\n",
        "We will provide the boilerplate for the PPO update loop, which often involves multiple optimization steps on the same batch of data.\n",
        "\n",
        "Your task is to implement the policy loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spzXcZrJUoaJ"
      },
      "source": [
        "<details>\n",
        "<summary>💡 Hints</summary>\n",
        "\n",
        "*   1. The probability ratio `w_t` is calculated in log space for numerical stability. Use `torch.exp(new_log_probs - old_log_probs)`.\n",
        "\n",
        "*   2. For the first surrogate objective (`surr1`), simply multiply the ratio `w_t` by the `advantages`.\n",
        "\n",
        "*   2. For the second surrogate objective (`surr2`), first clip the ratio `w_t` using `torch.clamp()`. The min and max values for the clamp are `1 - self._clip_epsilon` and `1 + self._clip_epsilon`. Then multiply the result by the `advantages`.\n",
        "\n",
        "*   3. The PPO policy loss takes the element-wise minimum of `surr1` and `surr2`. Use `torch.min()` for this, then take the `.mean()`, and remember to negate the final result.\n",
        "\n",
        "*   4. Add the value loss (mean squared error with returns) as before.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUjFFVs_BO59"
      },
      "outputs": [],
      "source": [
        "class PPOAgent(A2CEntropyAgent):\n",
        "    \"\"\"Implements the Proximal Policy Optimization (PPO) algorithm.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: NN_Policy_Value,\n",
        "        value_loss_weight: float = 0.5,\n",
        "        entropy_weight: float = 0.01,\n",
        "        clip_epsilon: float = 0.2,\n",
        "        n_epochs: int = 4,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy=policy,\n",
        "            value_loss_weight=value_loss_weight,\n",
        "            entropy_weight=entropy_weight,\n",
        "        )\n",
        "        self._clip_epsilon = clip_epsilon\n",
        "        self._n_epochs = n_epochs\n",
        "\n",
        "    def update(self, batch_data: dict, optimizer: torch.optim.Optimizer):\n",
        "        \"\"\"Performs the PPO update over multiple epochs.\"\"\"\n",
        "        # PPO uses the collected data multiple times, so we detach it.\n",
        "        old_log_probs = torch.cat(batch_data['log_probs']).detach()\n",
        "        old_values = torch.cat(batch_data['values']).detach()\n",
        "        returns = torch.cat(batch_data['returns']).detach()\n",
        "        states = torch.cat(batch_data['states']).detach()\n",
        "        actions = torch.tensor(sum(batch_data['actions'], [])).detach()\n",
        "\n",
        "        # Calculate advantages once using old values and normalize them.\n",
        "        advantages = returns - old_values\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # --- PPO Inner Loop ---\n",
        "        for _ in range(self._n_epochs):\n",
        "            # Get new predictions from the current (updated) policy.\n",
        "            new_logits, new_values = self.policy(states)\n",
        "            new_values = new_values.squeeze(-1)\n",
        "            distribution = Categorical(logits=new_logits)\n",
        "            new_log_probs = distribution.log_prob(actions)\n",
        "\n",
        "            ################################################################\n",
        "            # Your solution:\n",
        "            # 1. Calculate the importance sampling weights.\n",
        "            # w_t = ...\n",
        "\n",
        "            # 2. Calculate the two surrogate objectives.\n",
        "            # surr1 = ...\n",
        "            # surr2 = ...\n",
        "\n",
        "            # 3. Put together the PPO policy loss based on the two surrogates.\n",
        "            # policy_loss = ...\n",
        "\n",
        "            # 4. Value loss.\n",
        "            # value_loss = ...\n",
        "            ################################################################\n",
        "\n",
        "            # Entropy bonus.\n",
        "            entropy_bonus = distribution.entropy().mean()\n",
        "\n",
        "            loss = (\n",
        "                policy_loss\n",
        "                + self._value_loss_weight * value_loss\n",
        "                - self._entropy_weight * entropy_bonus\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZz6dUABoeN"
      },
      "source": [
        "Finally, let's adapt our `train_agent` function from Practical 1 to handle the Actor-Critic agents. The main change is handling the \"bootstrapped\" value from the final state if an episode is truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HcfGcNmC-NG"
      },
      "outputs": [],
      "source": [
        "# @title Training loop for Actor-Critic agents.\n",
        "\n",
        "def train_actor_critic_agent(\n",
        "    env: gym.Env,\n",
        "    agent: A2CAgent,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    num_updates: int,\n",
        "    episodes_per_batch: int,\n",
        "    max_episode_steps: int,\n",
        "    convergence_threshold: float,\n",
        "    log_frequency: int = 20\n",
        "):\n",
        "    \"\"\"A generic function to train Actor-Critic style agents.\"\"\"\n",
        "    avg_rewards_history = []\n",
        "    print(f\"Starting training for {agent.__class__.__name__}...\")\n",
        "\n",
        "    pbar = tqdm(range(num_updates), mininterval=2)\n",
        "    for i in pbar:\n",
        "        batch_data = {  # this is to hold all data for the batch.\n",
        "            'states': [], 'actions': [], 'rewards': [],\n",
        "            'log_probs': [], 'values': [], 'returns': []\n",
        "        }\n",
        "        batch_total_rewards = []\n",
        "\n",
        "        # --- Data Collection Phase ---\n",
        "        for _ in range(episodes_per_batch):\n",
        "            # Per-episode storage\n",
        "            episode_states, episode_actions, episode_rewards = [], [], []\n",
        "            episode_log_probs, episode_values = [], []\n",
        "            episode_total_reward = 0\n",
        "            state, _ = env.reset()\n",
        "\n",
        "            for step in range(max_episode_steps):\n",
        "                state_tensor = torch.tensor(state)\n",
        "                # Get action, log_prob, and value from the agent.\n",
        "                action, log_prob, value = agent(state_tensor)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Store experience.\n",
        "                episode_states.append(state_tensor)\n",
        "                episode_actions.append(action)\n",
        "                episode_rewards.append(reward)\n",
        "                episode_log_probs.append(log_prob)\n",
        "                episode_values.append(value)\n",
        "                episode_total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # --- Episode Finish: Calculate Returns and Bootstrap ---\n",
        "            last_value = 0.0\n",
        "            if truncated: # bootstrap if episode was cut short.\n",
        "                with torch.no_grad():\n",
        "                    _, last_value_tensor = agent.policy(torch.tensor(state))\n",
        "                    last_value = last_value_tensor.item()\n",
        "\n",
        "            # Calculate discounted returns for the episode.\n",
        "            returns = []\n",
        "            R = last_value\n",
        "            for r in reversed(episode_rewards):\n",
        "                R = r + agent.gamma * R\n",
        "                returns.insert(0, R)\n",
        "\n",
        "            # Add this episode's data to the batch.\n",
        "            batch_data['states'].append(torch.stack(episode_states))\n",
        "            batch_data['actions'].append(episode_actions)\n",
        "            batch_data['rewards'].append(episode_rewards)\n",
        "            batch_data['log_probs'].append(torch.stack(episode_log_probs))\n",
        "            batch_data['values'].append(torch.stack(episode_values))\n",
        "            batch_data['returns'].append(torch.tensor(returns, dtype=torch.float32))\n",
        "            batch_total_rewards.append(episode_total_reward)\n",
        "\n",
        "        # --- Learning Phase ---\n",
        "        agent.update(batch_data, optimizer)\n",
        "\n",
        "        # --- Logging ---\n",
        "        avg_reward_for_batch = np.mean(batch_total_rewards)\n",
        "        avg_rewards_history.append(avg_reward_for_batch)\n",
        "        pbar.set_postfix(avg_reward=f\"{avg_reward_for_batch:.2f}\")\n",
        "\n",
        "        # Check for convergence.\n",
        "        if avg_reward_for_batch > convergence_threshold:\n",
        "            print(f\"\\nSolved at update {i}!\")\n",
        "            break\n",
        "\n",
        "    return avg_rewards_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVqwm2GCBo16"
      },
      "outputs": [],
      "source": [
        "# --- Hyperparameters ---\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_UPDATES = 1000\n",
        "EPISODES_PER_BATCH = 10\n",
        "MAX_EPISODE_STEPS = 500\n",
        "\n",
        "VALUE_WEIGHT_LOSS = 0.5  # for A2C\n",
        "ENTROPY_WEIGHT = 0.01  # for A2C with regularization\n",
        "CLIP_EPSILON = 0.2  # for PPO only\n",
        "N_EPOCHS = 4  # for PPO only\n",
        "\n",
        "# --- Agent Selection ---\n",
        "agent_to_train = \"A2CEntropy\" # @param [\"A2C\", \"A2CEntropy\", \"PPO\"]\n",
        "\n",
        "# --- Initialization ---\n",
        "# We use the same policy network for all agents\n",
        "policy = NN_Policy_Value(input_dim=input_dim, num_actions=num_actions)\n",
        "\n",
        "if agent_to_train == \"A2C\":\n",
        "    agent = A2CAgent(\n",
        "        policy=policy,\n",
        "        value_loss_weight=VALUE_WEIGHT_LOSS,\n",
        "    )\n",
        "elif agent_to_train == \"A2CEntropy\":\n",
        "    agent = A2CEntropyAgent(\n",
        "        policy=policy,\n",
        "        value_loss_weight=VALUE_WEIGHT_LOSS,\n",
        "        entropy_weight=ENTROPY_WEIGHT,\n",
        "    )\n",
        "elif agent_to_train == \"PPO\":\n",
        "    agent = PPOAgent(\n",
        "        policy=policy,\n",
        "        value_loss_weight=VALUE_WEIGHT_LOSS,\n",
        "        entropy_weight=ENTROPY_WEIGHT,\n",
        "        clip_epsilon=CLIP_EPSILON,\n",
        "        n_epochs=N_EPOCHS,\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f'Unknown agent {agent_to_train}.')\n",
        "\n",
        "optimizer = optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# --- Run Training ---\n",
        "\n",
        "avg_rewards = train_actor_critic_agent(\n",
        "    env=env,\n",
        "    agent=agent,\n",
        "    optimizer=optimizer,\n",
        "    num_updates=NUM_UPDATES,\n",
        "    episodes_per_batch=EPISODES_PER_BATCH,\n",
        "    max_episode_steps=MAX_EPISODE_STEPS,\n",
        "    convergence_threshold=500,\n",
        "    log_frequency=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xZ50eqBpDPFE"
      },
      "outputs": [],
      "source": [
        "#@title Plotting Episodic Rewards\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(avg_rewards)\n",
        "plt.title(f\"Learning curve for {agent.__class__.__name__}\")\n",
        "plt.xlabel(\"Update Batch\")\n",
        "plt.ylabel(\"Average Reward per Batch\")\n",
        "plt.grid(True)\n",
        "# Add a line for the optimal score.\n",
        "plt.axhline(y=-13, color='r', linestyle='--', label='Optimal Score (-13)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzuVCfvSByb-"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the trained Agent.\n",
        "\n",
        "# We re-use the visualise_policy function from Practical 1.\n",
        "visualise_policy(\n",
        "    env_id='CartPole-v1',\n",
        "    policy=agent.policy,\n",
        "    max_episode_steps=500,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Ib-p99o7uw"
      },
      "source": [
        "# 🛠️ Practical 3: Fine-tuning a language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9dOWDuI8PFu"
      },
      "source": [
        "We have journeyed from simple tabular policies to neural networks solving classic control problems. Now, we'll scale up dramatically to one of the most exciting applications of RL today: fine-tuning Large Language Models (LLMs).\n",
        "\n",
        "You've likely heard of models like ChatGPT, Claude, Gemini and Llama. A key reason for their success is a process called **Reinforcement Learning from Human Feedback (RLHF)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPRnYMARDyID"
      },
      "source": [
        "#### Mapping RL Concepts to LLMs\n",
        "\n",
        "Let's connect this back to what we've learned. How do our RL concepts apply here?\n",
        "\n",
        "-   🧠 **Policy**: The LLM (GPT-2) is our policy. Its parameters are the weights of the neural network.\n",
        "-   ✍️ **Action**: The \"action\" is the generation of the next token from the entire vocabulary. A full response is a sequence of actions.\n",
        "-   📄 **State**: The \"state\" is the sequence of tokens generated so far. It starts with the input prompt and grows with each token the policy generates.\n",
        "-   👍 **Reward**: The score given to the final, complete response by our reward model (the sentiment classifier).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5L4fQ0E8Rx5"
      },
      "source": [
        "#### Why do we need RL for LLMs?\n",
        "\n",
        "Standard LLMs are trained to predict the next word in a massive dataset of text from the internet. This makes them excellent at mimicking human language, but it doesn't guarantee they are helpful, harmless, or follow instructions well. This is known as the **alignment problem**.\n",
        "\n",
        "RLHF is a technique to \"steer\" an already pre-trained LLM towards desired behaviors (like being more helpful, positive, or concise).\n",
        "\n",
        "The full RLHF process has three main steps:\n",
        "1.  **Pre-training an LLM**: This is the standard language modeling step (already done for us).\n",
        "2.  **Training a Reward Model (RM)**: A separate model is trained to predict which of two responses a human would prefer. This RM learns to score text based on human values.\n",
        "3.  **Fine-tuning with RL**: The pre-trained LLM (the policy) is then fine-tuned using RL. It generates text, the Reward Model scores it, and an RL algorithm (like PPO) updates the LLM's weights to maximize the reward.\n",
        "\n",
        "For this tutorial, we will implement a simplified version of this pipeline called **Reinforcement Learning from Classifier Feedback**. Instead of training our own reward model on human preferences, we will use a pre-trained sentiment **classifier** as our reward model. Our goal: to tune GPT-2 to generate more positive movie reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZoUIhmTDyOF"
      },
      "source": [
        "#### From REINFORCE to PPO\n",
        "\n",
        "For a complex task like language generation, the high variance of the REINFORCE algorithm can make training unstable. We'll use a more advanced and robust algorithm from the same policy gradient family: **Proximal Policy Optimization (PPO)**.\n",
        "\n",
        "> **Intuition for PPO**: You can think of PPO as a smarter, more stable version of REINFORCE. It also updates the policy based on rewards, but it constrains how much the policy can change in each update step (compared to the previous policy), preventing it from straying too far from what it already knows. This leads to much smoother training.\n",
        "\n",
        "We will use the [trl](https://github.com/huggingface/trl) library from Hugging Face, which handles the complexity of PPO for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eugoLm7ZrZQ"
      },
      "source": [
        "## GPT2 for positive reviews\n",
        "\n",
        "Let's fine-tune a pre-trained GPT-2 to generate positive continuations for movie review prompts. We'll use:\n",
        "*   **Policy**: A GPT-2 model fine-tuned on the IMDB dataset, called [lvwerra/gpt2-imdb](https://huggingface.co/lvwerra/gpt2-imdb) with 124M parameters.\n",
        "*   **Reward Model**: A DistilBERT model pre-trained on the sentiment analysis task: it has been trained to predict whether the text is positive or negative -- this has 66M parameters.\n",
        "\n",
        "**N.B.** This section is based on (now deprecated) [Tune GPT2 to generate positive reviews](https://huggingface.co/docs/trl/v0.1.1/en/sentiment_tuning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9hGVFmwiJ-4"
      },
      "outputs": [],
      "source": [
        "# Set up the device and a separate device for the reward pipeline.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipe_device = 0 if torch.cuda.is_available() else -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extI12wSDJyC"
      },
      "source": [
        "#### Configuration\n",
        "\n",
        "First, let's set up our environment and all the hyperparameters for our experiment:\n",
        "-  `config` for fine-tuning the pretrained LLM model using PPO\n",
        "- `sent_kwargs` for the reward (sentiment analysis) model\n",
        "- `gen_kwargs` for generating data with the LLM, which can be considered as gathering the trajectories by sampling actions from the agent's policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXOK8KGmDqFT"
      },
      "outputs": [],
      "source": [
        "# PPO Configuration.\n",
        "config = {\n",
        "    \"model_name\": \"lvwerra/gpt2-imdb\",\n",
        "    \"steps\": 20000,\n",
        "    \"batch_size\": 256,\n",
        "    \"txt_in_min_len\": 2,\n",
        "    \"txt_in_max_len\": 8,\n",
        "    \"txt_out_min_len\": 4,\n",
        "    \"txt_out_max_len\": 16,\n",
        "    \"lr\": 1.41e-5,\n",
        "    \"init_kl_coef\": 0.2,\n",
        "    \"target\": 6,\n",
        "}\n",
        "\n",
        "# Generation settings for the LLM.\n",
        "gen_kwargs = {\n",
        "    \"min_length\":-1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": None,\n",
        "}\n",
        "\n",
        "# Reward model settings.\n",
        "sent_kwargs = {\n",
        "    \"top_k\": None,\n",
        "    \"function_to_apply\": \"none\",\n",
        "    \"batch_size\": 16,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0nXBSEpMLYh"
      },
      "source": [
        "#### Load data\n",
        "\n",
        "We'll load the IMDB dataset, which contains movie reviews. These reviews will serve as the initial \"state\" or \"prompt\" for our GPT-2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gSEOQsWDows"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset('imdb', split='train')\n",
        "ds = ds.rename_columns({'text': 'review', 'label': 'sentiment'})\n",
        "# Filter out very short reviews.\n",
        "ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAXiw9Dicwjj"
      },
      "source": [
        "#### Load the reward model.\n",
        "\n",
        "Next, we'll load our reward model (the sentiment classifier). This pipeline will take a piece of text and return a score for \"positive\" and \"negative\" sentiment. We'll use the \"positive\" score as our reward signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59J0FFe_c2Mz"
      },
      "outputs": [],
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\"lvwerra/distilbert-imdb\", device=pipe_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og3GtzJ0BuxX"
      },
      "outputs": [],
      "source": [
        "# Let's test the reward pipeline -- feel free to try out your own examples!\n",
        "\n",
        "example_texts = [\n",
        "    \"This movie was absolutely fantastic, a must-see!\",\n",
        "    \"I was so bored I fell asleep in the theater.\"\n",
        "]\n",
        "\n",
        "rewards = []\n",
        "outputs = sentiment_pipe(example_texts, top_k=None)\n",
        "\n",
        "for output in outputs:\n",
        "    positive_score = output[1]['score']\n",
        "    rewards.append(positive_score)\n",
        "\n",
        "for example_text, reward in zip(example_texts, rewards):\n",
        "  print(f\"Text: '{example_text}'\\nReward: {reward:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLwAOPB_NxKi"
      },
      "source": [
        "#### Load policy model\n",
        "\n",
        "Now, we load our policy model (GPT-2). We need 2 copies:\n",
        "\n",
        "*  `gpt2_model`: This is the policy we will actively train and update.\n",
        "*  `gpt2_model_ref`: This is a frozen, non-trainable copy. PPO uses this reference to calculate the KL divergence penalty, ensuring our active policy doesn't stray too far from its original behavior.\n",
        "\n",
        "The `AutoModelForCausalLMWithValueHead` class from `trl` is a special wrapper that adds an extra \"value head\" to the standard GPT-2 model. This value head is trained to predict the expected reward from a given state, which is a key component of PPO's optimization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg7tPLJ9Dv0_"
      },
      "outputs": [],
      "source": [
        "gpt2_model = AutoModelForCausalLMWithValueHead.from_pretrained(config['model_name'])\n",
        "gpt2_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(config['model_name'])\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "# Set the pad token to the end-of-sequence token.\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "gen_kwargs[\"pad_token_id\"] = gpt2_tokenizer.pad_token_id\n",
        "\n",
        "gpt2_model.to(device);\n",
        "gpt2_model_ref.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQQb-WSOHmX"
      },
      "source": [
        "#### Pre-process Data and Initialize PPO\n",
        "\n",
        "We need to prepare the data for the `PPOTrainer`. We will tokenize the prompts from our dataset. To make training more robust, we'll use `LengthSampler` to generate prompts and responses of varying lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FVPKuEOD0rP"
      },
      "outputs": [],
      "source": [
        "class LengthSampler:\n",
        "    def __init__(self, min_value, max_value):\n",
        "        self.values = list(range(min_value, max_value))\n",
        "    def __call__(self):\n",
        "        return np.random.choice(self.values)\n",
        "\n",
        "# Sampler for dynamic query and response lengths\n",
        "input_size = LengthSampler(config['txt_in_min_len'], config['txt_in_max_len'])\n",
        "output_size = LengthSampler(config['txt_out_min_len'], config['txt_out_max_len'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqNxOkghD1IW"
      },
      "outputs": [],
      "source": [
        "def tokenize(sample):\n",
        "    sample[\"tokens\"] = gpt2_tokenizer.encode(sample[\"review\"])[:input_size()]\n",
        "    sample[\"query\"] = gpt2_tokenizer.decode(sample[\"tokens\"])\n",
        "    return sample\n",
        "\n",
        "ds = ds.map(tokenize, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3_xeS-5D4x2"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(ds, batch_size=config['batch_size'], collate_fn=collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmQqBeRkeXjb"
      },
      "source": [
        "Finally, we initialize the `PPOTrainer`. This powerful class from `trl` orchestrates the entire fine-tuning process. We pass it our models, tokenizer, and the PPO configuration we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOJ0OTSUiszn"
      },
      "outputs": [],
      "source": [
        "ppo_config = PPOConfig(batch_size=config['batch_size'], mini_batch_size=16)\n",
        "ppo_trainer = PPOTrainer(ppo_config, gpt2_model, gpt2_model_ref, gpt2_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fngcgHGwUbky"
      },
      "source": [
        "### The Fine-tuning Loop\n",
        "---\n",
        "\n",
        "The training loop is where the magic happens. For each batch of prompts, we perform a three-step process:\n",
        "1.  **Get Responses**: Use the policy model (`gpt2_model`) to generate completions for the prompts.\n",
        "2.  **Get Rewards**: Use the sentiment classifier (`sentiment_pipe`) to score the completions.\n",
        "3.  **Optimize**: Feed the prompts, responses, and rewards into the `ppo_trainer` to update the policy model's weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzPjmA5CfZf6"
      },
      "source": [
        "#### ✏️ **Exercise: Implement the PPO training step!**\n",
        "\n",
        "Complete the code in the cell below. You'll need to implement the three steps described above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMXGvqwoflBo"
      },
      "source": [
        "<details><summary>💡 Hints</summary>\n",
        "\n",
        "1.  **Generate Responses**:\n",
        "    *   The `query_tensors` (the tokenized prompts) are already prepared for you.\n",
        "    *   You need to loop through them and use `gpt2_model.generate()` to create a response for each one.\n",
        "    *   Remember to use a random `output_size()` for each generation and pass the `gen_kwargs`.\n",
        "    *   The generated output from the model includes the prompt, so you'll need to slice it to get only the new tokens (the response). The length of the response is `gen_len`.\n",
        "\n",
        "2.  **Calculate Rewards**:\n",
        "    *   The reward is calculated on the full text (`query + response`). You'll need to create a list of these full texts.\n",
        "    *   Pass this list to the `sentiment_pipe`.\n",
        "    *   The output of the pipeline is a list of dictionaries. For each output, you want the score for the 'POSITIVE' label. In our case, the pipeline returns `[{'label': 'NEGATIVE', 'score': ...}, {'label': 'POSITIVE', 'score': ...}]`, so you'll want `output[1][\"score\"]`.\n",
        "    *   Make sure your final `rewards` are a list of PyTorch tensors.\n",
        "\n",
        "3.  **Perform PPO Step**:\n",
        "    *   The final step is easy! Just call `ppo_trainer.step()` and pass it the `query_tensors`, `response_tensors`, and `rewards` you prepared.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw8i7RQtD_cC"
      },
      "outputs": [],
      "source": [
        "# Set the number of epochs for fine-tuning.\n",
        "# A few epochs are enough to see a change.\n",
        "MAX_NUM_EPOCHS = 5  # @ param\n",
        "\n",
        "total_ppo_epochs = int(np.ceil(config['steps'] / config['batch_size']))\n",
        "\n",
        "\n",
        "for epoch, batch in tqdm(zip(range(total_ppo_epochs), iter(dataloader))):\n",
        "  ################################################################\n",
        "  # Fine-tune GPT2 using the pre-trained distilbert-imdb as the reward function.\n",
        "  # Your solution:\n",
        "  # Get the query tensors using batch['tokens'].\n",
        "  # query_tensors = ...\n",
        "\n",
        "  # Get response from gpt2\n",
        "  # response_tensors = []\n",
        "  # for i in range(config['batch_size']):\n",
        "  # Use gpt2_model.generate() to get the responses.\n",
        "  # response = ...\n",
        "  # response_tensors.append(...)\n",
        "  # batch['response'] = [gpt2_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "  # Compute sentiment score, concatenating batch['query'] and batch['response']\n",
        "  # to build the input for sentiment_pipe.\n",
        "  # ...\n",
        "  # rewards = ...\n",
        "\n",
        "  # Run PPO step.\n",
        "  # stats = ppo_trainer.step(...)\n",
        "  ################################################################\n",
        "  logs, timing = dict(), dict()\n",
        "  query_tensors = [torch.tensor(t).long().to(device) for t in batch['tokens']]\n",
        "\n",
        "  # Get response from gpt2.\n",
        "  response_tensors = []\n",
        "  for i in range(config['batch_size']):\n",
        "    gen_len = output_size()\n",
        "    ################################################################\n",
        "    # response = ...\n",
        "    ################################################################\n",
        "    response_tensors.append(response.squeeze()[-gen_len:])\n",
        "  batch['response'] = [\n",
        "      gpt2_tokenizer.decode(r.squeeze()) for r in response_tensors\n",
        "  ]\n",
        "\n",
        "  # Compute sentiment score.\n",
        "  texts = [q + r for q, r in zip(batch['query'], batch['response'])]\n",
        "  ################################################################\n",
        "  # pipe_outputs = ...\n",
        "  ################################################################\n",
        "  rewards = [\n",
        "      torch.tensor(output[1]['score']).to(device) for output in pipe_outputs\n",
        "  ]\n",
        "\n",
        "  # Run PPO step.\n",
        "  ################################################################\n",
        "  # stats = ...\n",
        "  ################################################################\n",
        "  rewards = torch.tensor(rewards).to(device)\n",
        "\n",
        "  table_rows = [\n",
        "      list(r)\n",
        "      for r in zip(batch['query'], batch['response'], rewards.cpu().tolist())\n",
        "  ]\n",
        "  logs.update(timing)\n",
        "  logs.update(stats)\n",
        "  logs['env/reward_mean'] = torch.mean(rewards).cpu().numpy()\n",
        "  logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
        "  logs['env/reward_dist'] = rewards.cpu().numpy()\n",
        "\n",
        "  if epoch == MAX_NUM_EPOCHS:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xccr6OGKioW"
      },
      "source": [
        "#### Evaluate the Results\n",
        "\n",
        "Now, let's see if our fine-tuning worked! We will take a sample of prompts and generate responses from both the original model (`gpt2_model_ref`) and our fine-tuned model (`gpt2_model`). We can then compare them qualitatively by reading them, and quantitatively by checking their average reward scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRfVYBpzgS1r"
      },
      "outputs": [],
      "source": [
        "# Get a batch from the dataset.\n",
        "bs = 16  # a batch size\n",
        "game_data = dict()\n",
        "ds.set_format('pandas')\n",
        "df_batch = ds[:].sample(bs)\n",
        "game_data['query'] = df_batch['query'].tolist()\n",
        "query_tensors = df_batch['tokens'].tolist()\n",
        "\n",
        "response_tensors_ref, response_tensors = [], []\n",
        "\n",
        "# Get response from `gpt2` and `gpt2_ref`.\n",
        "for i in range(bs):\n",
        "  gen_len = output_size()\n",
        "  output = gpt2_model_ref.generate(\n",
        "      torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "      max_new_tokens=gen_len,\n",
        "      **gen_kwargs,\n",
        "  ).squeeze()[-gen_len:]\n",
        "  response_tensors_ref.append(output)\n",
        "  output = gpt2_model.generate(\n",
        "      torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device),\n",
        "      max_new_tokens=gen_len,\n",
        "      **gen_kwargs,\n",
        "  ).squeeze()[-gen_len:]\n",
        "  response_tensors.append(output)\n",
        "\n",
        "# Decode responses.\n",
        "game_data['response (before)'] = [\n",
        "    gpt2_tokenizer.decode(response_tensors_ref[i]) for i in range(bs)\n",
        "]\n",
        "game_data['response (after)'] = [\n",
        "    gpt2_tokenizer.decode(response_tensors[i]) for i in range(bs)\n",
        "]\n",
        "\n",
        "# Sentiment analysis of query/response pairs before/after.\n",
        "texts = [\n",
        "    q + r for q, r in zip(game_data['query'], game_data['response (before)'])\n",
        "]\n",
        "game_data['rewards (before)'] = [\n",
        "    output[1]['score'] for output in sentiment_pipe(texts, **sent_kwargs)\n",
        "]\n",
        "\n",
        "texts = [\n",
        "    q + r for q, r in zip(game_data['query'], game_data['response (after)'])\n",
        "]\n",
        "game_data['rewards (after)'] = [\n",
        "    output[1]['score'] for output in sentiment_pipe(texts, **sent_kwargs)\n",
        "]\n",
        "\n",
        "# Store results in a dataframe.\n",
        "df_results = pd.DataFrame(game_data)\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean:')\n",
        "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
        "print()\n",
        "print('Median:')\n",
        "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
      ],
      "metadata": {
        "id": "Am5Z-USRSnU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEh68OxHPQmO"
      },
      "source": [
        "You should see a noticeable increase in the average reward, even after fine-tuning for just a few epochs! When you look at the generated text, you'll likely see that the \"after\" responses are more consistently positive in tone.\n",
        "\n",
        "This demonstrates the core loop of RLHF: **generate, reward, and optimize**. You've successfully steered an LLM's behavior using a reward signal, bridging the gap from simple RL problems to the cutting edge of AI alignment.\n",
        "\n",
        "Feel free to experiment with different reward functions or different LLMs! 😀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhqhahWgFVRs"
      },
      "source": [
        "## Beyond Classifiers: A Spectrum of Feedback\n",
        "\n",
        "In our practical exercise, we used a pre-trained sentiment classifier to provide a reward signal. This is an example of machine feedback, but it's just one point on a wide spectrum of methods used to score LLM responses.\n",
        "\n",
        "### **🧑‍⚖️ RLHF: Human Feedback**\n",
        "\n",
        "This is the \"gold standard\" for capturing nuanced, subjective, and value-laden concepts that are difficult to define with rules.\n",
        "\n",
        "**How it Works**: The process focuses on learning preferences.\n",
        "1.  A prompt is given to the LLM, which generates two or more different responses (e.g., Response A, Response B).\n",
        "2.  A human labeler reads the prompt and the responses and indicates which one they prefer (e.g., `A` is better than `B`).\n",
        "3.  This process is repeated thousands of times, creating a large dataset of human preferences.\n",
        "4.  A separate reward model (RM) is trained on this dataset to predict which response a human would prefer. The RM learns to output a scalar score (a reward) that reflects this preference.\n",
        "\n",
        "**Where it Excels**:\n",
        "-   **Subjectivity & Style**: Teaching an AI to be funny, poetic, or empathetic.\n",
        "-   **Helpfulness & Harmlessness**: Aligning models with complex human values that go beyond simple right/wrong answers.\n",
        "-   **Summarization**: Creating summaries that are not just factually correct but also intuitive and easy to understand.\n",
        "\n",
        "### **🤖 RLMF: Machine Feedback**\n",
        "\n",
        "This is the method we used in our tutorial. The reward signal comes from an automated, objective, and verifiable source.\n",
        "\n",
        "**How it Works**: The reward function is an explicit program or model that checks for correctness based on predefined rules.\n",
        "    -   A sentiment classifier checks for positive language.\n",
        "    -   A unit test suite checks if generated code compiles and runs correctly.\n",
        "    -   A validator checks if the output conforms to a specific format (e.g., valid JSON).\n",
        "\n",
        "**Where it Excels**:\n",
        "-   **Coding**: The reward can be `+1` if the code passes all unit tests, and `0` otherwise.\n",
        "-   **Mathematics**: The reward is high if the model's final answer matches the correct solution.\n",
        "-   **Structured Data Generation**: The reward is high if the output is a perfectly formatted JSON object or XML file.\n",
        "\n",
        "### **📜 RLAIF: AI Feedback**\n",
        "\n",
        "This approach aims to solve the biggest bottleneck of RLHF: the time and cost of collecting human feedback. Instead of humans, it uses a powerful \"critic\" LLM to provide the preference labels.\n",
        "\n",
        "**How it Works**: This is the core idea behind **Constitutional AI**, pioneered by Anthropic.\n",
        "1.  A \"constitution\" is written. This is a set of principles and rules for the desired AI behavior (e.g., \"Choose the response that is more helpful and less harmful,\" \"Avoid manipulative or toxic language\").\n",
        "2.  The LLM generates two responses (A and B) to a prompt.\n",
        "3.  A powerful, separate \"critic\" LLM is prompted to evaluate both responses *according to the constitution* and explain which one is better.\n",
        "4.  This creates an AI-generated preference label (`A > B`), which is then used to train a Reward Model, just as human labels would be.\n",
        "\n",
        "**Where it Excels**:\n",
        "-   **Scalability**: Drastically reduces the need for human labelers, allowing for much larger preference datasets.\n",
        "-   **Safety & Ethics**: Excellent for enforcing explicit ethical principles and reducing harmful, toxic, or biased outputs at scale.\n",
        "-   **Transparency**: The reasoning for a preference is tied back to an explicit principle in the constitution.\n",
        "\n",
        "| Feedback Type      | Reward Source                                     | Pros                                                 | Excels In                                             |\n",
        "| ------------------ | ------------------------------------------------- | ---------------------------------------------------- | ----------------------------------------------------- |\n",
        "| RL**H**F   | Human preference labels                           | Great for nuance, values, and quality | Subjective style, creativity, harmlessness |\n",
        "| RL**M**F | A program, classifier, or set of rules           | Objective, fast, cheap, and verifiable            | Coding, math, structured data formatting             |\n",
        "| RL**AI**F     | An LLM \"critic\" guided by a constitution         | Scalable, enforces explicit principles       | Safety alignment, reducing toxicity, ethical scaling |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFQtacEHhWS"
      },
      "source": [
        "## Beyond PPO: other RL algorithms for LLMs\n",
        "\n",
        "We used PPO, a powerful and trusted algorithm from the `trl` library. However, the field is moving at an incredible pace, and researchers are constantly developing simpler, more efficient, and more stable alternatives to PPO. Here are a few of the most important recent developments.\n",
        "\n",
        "### **🎯 DPO** (Direct Preference Optimization)\n",
        "\n",
        "DPO has rapidly become a popular alternative to PPO due to its simplicity and stability. It was a breakthrough that showed you don't actually need an explicit, separately trained reward model.\n",
        "\n",
        "**Core Idea**: DPO cleverly reframes the problem. Instead of the two-stage process of \"1. Train a Reward Model, then 2. Use RL to satisfy it,\" DPO directly fine-tunes the policy on the preference data itself. It uses a loss function that directly increases the probability of preferred responses and decreases the probability of dispreferred ones, while a KL-divergence term keeps the policy from straying too far from a reference model.\n",
        "\n",
        "**Why It's a Big Deal**:\n",
        "-   **Simpler**: It completely eliminates the need to train, store, and run inference with a separate reward model, cutting the complexity of the RLHF pipeline in half.\n",
        "-   **More Stable**: It avoids the common RL problem where the policy finds an exploit in a flawed reward model. By training directly on preferences, it's more robust.\n",
        "\n",
        "**Further Reading**: [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)\n",
        "\n",
        "\n",
        "### **🎲 RLOO** (REINFORCE Leave-One-Out)\n",
        "\n",
        "This algorithm goes \"back to basics,\" showing that the simple REINFORCE algorithm we learned earlier can be made powerful enough for modern LLMs with a clever trick.\n",
        "\n",
        "**Core Idea**: The main weakness of vanilla REINFORCE is the high variance of its reward signal. RLOO tackles this by creating a much better baseline for calculating the advantage. For a given prompt, it generates *multiple* responses. To calculate the advantage for any single response, it uses the average reward of **all other \"sibling\" responses** as a baseline. A response is only positively reinforced if it's better than the average of its peers generated from the same prompt.\n",
        "\n",
        "**Why It's a Big Deal**: It makes the simple and elegant REINFORCE algorithm viable for LLM fine-tuning. It's much simpler to implement than PPO because it doesn't require a separate critic/value function network, yet it achieves competitive performance by drastically reducing variance.\n",
        "\n",
        "**Further Reading**: [Back to Basics: Revisiting REINFORCE-Style Optimization for Learning from Human Feedback in LLMs](https://arxiv.org/abs/2402.14740)\n",
        "\n",
        "\n",
        "### **🏅 GRPO** (Group Relative Policy Optimization)\n",
        "\n",
        "Developed for the high-performing **DeepSeek-Math** model, GRPO is a specialized algorithm designed for tasks where there can be multiple correct solutions of varying quality, such as mathematical reasoning.\n",
        "\n",
        "**Core Idea**: GRPO generalizes DPO from handling a *pair* of responses to handling a *group* of ranked responses. For a single math problem, the model might generate several different potential solutions. These solutions are then scored by a verifier (e.g., a Python interpreter that checks the final answer). GRPO's loss function then trains the policy to understand the *relative ranking* of the entire group—it learns to assign a higher probability to the best solution than the second-best, a higher probability to the second-best than the third-best, and so on.\n",
        "\n",
        "**Why It's a Big Deal**: It is extremely effective for complex reasoning tasks. By generating and learning from a diverse group of candidate solutions, the model is more likely to discover a correct pathway. GRPO then efficiently extracts the maximum amount of learning signal from this entire group, far more than a simple pairwise preference could provide.\n",
        "\n",
        "**Further Reading**: [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIX82F6zFVW7"
      },
      "source": [
        "## Final Words\n",
        "\n",
        "This concludes our journey into Reinforcement Learning!\n",
        "\n",
        "The principles of policy, reward, and optimization remain the same, whether you're solving a simple grid world or aligning a massive AI model. We hope you enjoyed this tutorial and are excited to see how you build on this knowledge! 💡🎓📚🧠"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}